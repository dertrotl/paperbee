# .github/workflows/research-papers.yml
name: PaperBee Daily Digest MarrPeng

on:
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6 AM UTC (8 AM German time during DST)
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 30s)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=30/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                with open(self.search_file_biorxiv) as papers_file:
                    articles_biorxiv_dict = json.load(papers_file)["papers"]
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"Found {len(articles_pub_arx_dict)} articles from PubMed/ArXiv")
            print(f"Found {len(articles_biorxiv_dict)} articles from bioRxiv")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied bioRxiv fix to {finder_file}")
                  else:
                      print(f"â„¹ï¸ bioRxiv fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {finder_file}: {e}")
          EOF
          
          # Fix XML parsing errors in DOI extraction
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for XML error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace the problematic XML parsing line
                  old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"âš ï¸ XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied XML error handling to {utils_file}")
                  else:
                      print(f"â„¹ï¸ XML error handling already applied or pattern not found in {utils_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini rate limiting: 30 RPM = 2 seconds minimum, use 3.5s for safety
                      time.sleep(3.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(3.5)' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied Gemini rate limiting to {llm_file}")
                  else:
                      print(f"â„¹ï¸ Rate limiting already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {llm_file}: {e}")
          EOF
          
      - name: Create config
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID_2 }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN_2 }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID_2 }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN_2 }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          
          # Define research query directly in workflow - same for all databases
          research_query = "[clonal hematopoiesis] OR [CHIP] OR [computational pathology] OR [cell segmentation] OR [hematology] OR [haematology] OR [blood disorders] OR [hematopoiesis] OR [haematopoiesis] OR [computational hematopathology] OR [single cell profiling] OR [mechanistic modeling] OR [mechanistic modelling] OR [clonal dynamics] OR [foundation models] OR [self-supervised learning] OR [multiple instance learning] OR [multi-omics] OR [multiomics] OR [microscopy image analysis] OR [computational biology] OR [machine learning pathology] OR [AI microscopy] OR [cell classification] OR [cell detection] OR [quantitative microscopy] OR [fluorescence microscopy] OR [cryo-electron microscopy] OR [image processing] OR [deep learning medical] OR [large language models healthcare] OR [AI healthcare] OR [mathematical modeling biology] OR [time-resolved modeling] OR [drug target identification] OR [biomedical image analysis] OR [segmentation] OR [3D segmentation] OR [vision language model]"
          
          # Define LLM filtering prompt directly in workflow
          filtering_prompt = """You are a lab manager at a research lab focusing on computational hematology, blood disorders, and microscopy image analysis, computational pathology and bioinformatics. The lab develops machine learning algorithms for cell segmentation, computational pathology, and single-cell profiling in hematological contexts. Key research areas include clonal hematopoiesis (CHIP), clonal dynamics, mechanistic mathematical modeling of haematopoiesis, and multi-omics approaches to blood cell production and disorders. The lab works extensively with foundation models, self-supervised learning, and multiple instance learning for analyzing microscopic images including bright-field, fluorescence, cryo-electron, and extended depth-of-field microscopy. Research focuses on developing AI-based algorithms for cell detection, classification, and quantification to advance diagnosis and treatment of severe blood disorders. Lab members are interested in computational hematopathology, single cell-based big data analysis for drug target identification, mechanistic modeling of time-resolved hematological data, and large language models/agents in healthcare applications. The lab combines AI with biomedical knowledge to classify individual cells and patients, with particular emphasis on quantitative microscopy image processing and custom machine learning methods for life scientists and pathologists. You are reviewing a list of research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?"""
          
          config = {
              "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              
              # Use same query for all databases
              "query": research_query,
              
              "LLM_FILTERING": True,
              "LLM_PROVIDER": "openai",
              "LANGUAGE_MODEL": "gemini-2.0-flash-lite",
              "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
              "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
              "FILTERING_PROMPT": filtering_prompt,
              
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              },
              "TELEGRAM": {"is_posting_on": False},
              "ZULIP": {"is_posting_on": False}
          }
          
          with open("config.yml", "w") as f:
              yaml.dump(config, f, default_flow_style=False)
          
          # Debug output
          print("Generated config:")
          print(f"Query: {research_query}")
          print("Using same query for all databases (PubMed, ArXiv, bioRxiv)")
          EOF
          
      - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "ðŸ”¬ Starting PaperBee Research Digest"
          echo "ðŸ“… Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "ðŸ”§ NCBI timeout: 30s | Gemini rate limit: 3.5s"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config.yml
