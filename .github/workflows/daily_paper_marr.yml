# .github/workflows/research-papers.yml
name: PaperBee Daily Digest MarrPeng

on:
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6 AM UTC (8 AM German time during DST)
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 30s)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=30/g' {} \;
          
          # Apply bioRxiv search fix + comprehensive debug
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply comprehensive fixes
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Force comprehensive debug regardless of existing patterns
                  # 1. Add debug at function start
                  if 'print("üîç LLM Debug Start")' not in content:
                      content = content.replace(
                          'def find_and_process_papers(self) -> pd.DataFrame:',
                          '''def find_and_process_papers(self) -> pd.DataFrame:
        print("üîç LLM Debug Start")
        print(f"‚úÖ self.llm_filtering = {self.llm_filtering}")
        print(f"‚úÖ self.llm_provider = {getattr(self, 'llm_provider', 'NOT_SET')}")
        print(f"‚úÖ self.model = {getattr(self, 'model', 'NOT_SET')}")'''
                      )
                  
                  # 2. Add debug after article processing
                  if 'print("üìã After ArticlesProcessor")' not in content:
                      content = content.replace(
                          'self.logger.info(f"Found {len(processed_articles)} articles.")',
                          '''self.logger.info(f"Found {len(processed_articles)} articles.")
        print("üìã After ArticlesProcessor")
        print(f"üìä Articles by source: {processed_articles['Source'].value_counts().to_dict() if len(processed_articles) > 0 else 'Empty'}")'''
                      )
                  
                  # 3. Force LLM debug regardless of existing patterns
                  if 'print("üß† CHECKING LLM FILTERING")' not in content:
                      # Find the LLM filtering section and add comprehensive debug
                      llm_section_start = content.find('if self.llm_filtering:')
                      if llm_section_start != -1:
                          # Insert debug right before the if statement
                          before_if = content[:llm_section_start]
                          after_if = content[llm_section_start:]
                          
                          debug_insert = '''        print("üß† CHECKING LLM FILTERING")
        print(f"ÔøΩ self.llm_filtering = {self.llm_filtering}")
        print(f"üîç type(self.llm_filtering) = {type(self.llm_filtering)}")
        print(f"üîç bool(self.llm_filtering) = {bool(self.llm_filtering)}")
        
        '''
                          content = before_if + debug_insert + after_if
                  
                  # 4. Add debug inside LLM filtering block
                  if 'print("üéØ INSIDE LLM BLOCK")' not in content:
                      content = content.replace(
                          'if self.llm_filtering:\n            llm_filter = LLMFilter(',
                          '''if self.llm_filtering:
            print("üéØ INSIDE LLM BLOCK - LLM filtering is TRUE!")
            print(f"üìä Processing {len(processed_articles)} articles with LLM")
            print(f"Article breakdown: {processed_articles['Source'].value_counts().to_dict()}")
            llm_filter = LLMFilter('''
                      )
                  
                  # 5. Add debug after LLM filtering
                  if 'print("‚úÖ LLM FILTERING COMPLETED")' not in content:
                      # Find the line right after llm_filter.filter_articles()
                      llm_filter_line = 'processed_articles = llm_filter.filter_articles()'
                      if llm_filter_line in content:
                          content = content.replace(
                              llm_filter_line,
                              llm_filter_line + '''
            print("‚úÖ LLM FILTERING COMPLETED")
            print(f"üìä After LLM: {len(processed_articles)} articles remain")
            if len(processed_articles) > 0:
                print(f"Final sources: {processed_articles['Source'].value_counts().to_dict()}")'''
                          )
                  
                  # 6. Add else block debug for when LLM is disabled
                  if 'print("‚ö†Ô∏è LLM FILTERING DISABLED")' not in content:
                      # Find the pattern after LLM filtering block
                      pattern_after_llm = 'self.logger.info(f"Filtered down to {len(processed_articles)} articles using LLM.")'
                      if pattern_after_llm in content:
                          content = content.replace(
                              pattern_after_llm,
                              pattern_after_llm + '''
        else:
            print("‚ö†Ô∏è LLM FILTERING DISABLED")
            print(f"Articles passing through unfiltered: {len(processed_articles)}")'''
                          )

                  with open(finder_file, 'w') as f:
                      f.write(content)
                  
                  print(f"‚úÖ Applied comprehensive debug to {finder_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {finder_file}: {e}")
          EOF
          
          # Fix XML parsing errors in DOI extraction
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for XML error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace the problematic XML parsing line
                  old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"‚ö†Ô∏è XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied XML error handling to {utils_file}")
                  else:
                      print(f"‚ÑπÔ∏è XML error handling already applied or pattern not found in {utils_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini rate limiting: 30 RPM = 2 seconds minimum, use 3.5s for safety
                      time.sleep(3.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(3.5)' not in content:
                      content = content.replace(old_pattern, new_pattern)
                  
                  # Add debug logging to LLM filtering
                  old_debug_pattern = 'def filter_articles(self):'
                  new_debug_pattern = '''def filter_articles(self):
          print(f"üß† LLM Filtering: Starting with {len(self.articles)} articles")
          print(f"Article sources: {self.articles['Source'].value_counts().to_dict()}")'''
                  
                  if old_debug_pattern in content and 'üß† LLM Filtering' not in content:
                      content = content.replace(old_debug_pattern, new_debug_pattern)
                  
                  # Add debug info in is_relevant method
                  old_relevant_pattern = 'def is_relevant('
                  new_relevant_pattern = '''def is_relevant('''
                  
                  # Add debug after successful LLM call
                  old_success_pattern = 'if response.choices[0].message.content.strip().lower() == "yes":'
                  new_success_pattern = '''print(f"ü§ñ LLM response for '{title[:30]}...': {response.choices[0].message.content.strip()}")
                  if response.choices[0].message.content.strip().lower() == "yes":'''
                  
                  if old_success_pattern in content and 'ü§ñ LLM response' not in content:
                      content = content.replace(old_success_pattern, new_success_pattern)
                  
                  # Add debug at end of filter_articles
                  old_return_pattern = 'return filtered_articles'
                  new_return_pattern = '''print(f"üß† LLM Filtering: Filtered down to {len(filtered_articles)} articles")
          print(f"Final article sources: {filtered_articles['Source'].value_counts().to_dict() if len(filtered_articles) > 0 else 'No articles'}")
          return filtered_articles'''
                  
                  if old_return_pattern in content and 'üß† LLM Filtering: Filtered down to' not in content:
                      content = content.replace(old_return_pattern, new_return_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied Gemini rate limiting and debug to {llm_file}")
                  else:
                      print(f"‚ÑπÔ∏è Rate limiting and debug already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {llm_file}: {e}")
          EOF
          
      - name: Create config
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID_2 }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN_2 }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID_2 }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN_2 }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          
          # Use single query for all databases (original working version)
          research_query = "[hematopoiesis] OR [haematopoiesis] OR [hematology] OR [haematology] OR [blood disorders] OR [computational pathology] OR [cell segmentation] OR [segmentation] OR [3D segmentation] OR [single cell] OR [single-cell] OR [machine learning] OR [deep learning] OR [artificial intelligence] OR [computer vision] OR [image analysis] OR [microscopy] OR [computational biology] OR [multi-omics] OR [multiomics] OR [foundation models] OR [self-supervised learning] OR [cell classification] OR [cell detection] OR [image processing] OR [biomedical image analysis] OR [medical imaging] OR [AI microscopy] OR [quantitative microscopy] OR [fluorescence microscopy] OR [vision language model] OR [large language models] OR [mathematical modeling] OR [mechanistic modeling] OR [drug target identification] OR [spatial transcriptomics] OR [pathology] OR [hematopathology] OR [blood cell] OR [leukemia] OR [lymphoma] OR [myeloma] OR [CHIP] OR [clonal hematopoiesis] OR [LLM] OR [language model]"
          
          # Define LLM filtering prompt directly in workflow (less restrictive)
          filtering_prompt = """You are a lab manager at a research lab focusing on computational hematology, blood disorders, and microscopy image analysis, computational pathology and bioinformatics. The lab develops machine learning algorithms for cell segmentation, computational pathology, and single-cell profiling in hematological contexts. Key research areas include clonal hematopoiesis (CHIP), clonal dynamics, mechanistic mathematical modeling of haematopoiesis, and multi-omics approaches to blood cell production and disorders. The lab works extensively with foundation models, self-supervised learning, and multiple instance learning for analyzing microscopic images including bright-field, fluorescence, cryo-electron, and extended depth-of-field microscopy. Research focuses on developing AI-based algorithms for cell detection, classification, and quantification to advance diagnosis and treatment of severe blood disorders. Lab members are interested in computational hematopathology, single cell-based big data analysis for drug target identification, mechanistic modeling of time-resolved hematological data, and large language models/agents in healthcare applications. The lab combines AI with biomedical knowledge to classify individual cells and patients, with particular emphasis on quantitative microscopy image processing and custom machine learning methods for life scientists and pathologists. 

You should be INCLUSIVE rather than exclusive in your filtering. Papers that are even tangentially related to machine learning, computational biology, single-cell analysis, hematology, pathology, or image analysis should be considered relevant. When in doubt, choose 'yes' rather than 'no'.

You are reviewing a list of research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?"""
          
          config = {
              "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              
              # Use separate queries for each database (MORE RELIABLE)
              "query_biorxiv": research_query,
              "query_pubmed_arxiv": research_query,
              
              "LLM_FILTERING": True,
              "LLM_PROVIDER": "openai",
              "LANGUAGE_MODEL": "gemini-2.0-flash-lite",
              "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
              "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
              "FILTERING_PROMPT": filtering_prompt,
              
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              },
              "TELEGRAM": {"is_posting_on": False},
              "ZULIP": {"is_posting_on": False}
          }
          
          with open("config.yml", "w") as f:
              yaml.dump(config, f, default_flow_style=False)
          
          # Debug output
          print("Generated config:")
          print(f"Query: {research_query}")
          print("Using same query for all databases (PubMed, ArXiv, bioRxiv)")
          print("Note: Broad queries for maximum coverage - LLM will filter for relevance")
          print(f"Query contains {len(research_query.split(' OR '))} search terms")
          print(f"LLM_FILTERING enabled: {config['LLM_FILTERING']}")
          print(f"LLM_PROVIDER: {config['LLM_PROVIDER']}")
          print(f"LANGUAGE_MODEL: {config['LANGUAGE_MODEL']}")
          EOF
          
      - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "üî¨ Starting PaperBee Research Digest"
          echo "üìÖ Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "üîß NCBI timeout: 30s | Gemini rate limit: 3.5s"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config.yml
