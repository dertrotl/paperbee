# .github/workflows/daily-papers-simple.yml
name: PaperBee Daily Digest Clinic

on:
  schedule:
    - cron: '0 7 * * 1-5'  # Monday-Friday at 7 AM UTC
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 30s)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=30/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                with open(self.search_file_biorxiv) as papers_file:
                    articles_biorxiv_dict = json.load(papers_file)["papers"]
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"Found {len(articles_pub_arx_dict)} articles from PubMed/ArXiv")
            print(f"Found {len(articles_biorxiv_dict)} articles from bioRxiv")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied bioRxiv fix to {finder_file}")
                  else:
                      print(f"â„¹ï¸ bioRxiv fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {finder_file}: {e}")
          EOF
          
          # Fix XML parsing errors in DOI extraction
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for XML error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace the problematic XML parsing line
                  old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"âš ï¸ XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied XML error handling to {utils_file}")
                  else:
                      print(f"â„¹ï¸ XML error handling already applied or pattern not found in {utils_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini 2.5 Flash Lite rate limiting: 15 RPM = 4s minimum, use 4.5s for safety
                      time.sleep(4.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied Gemini 2.5 Flash Lite rate limiting to {llm_file}")
                  else:
                      print(f"â„¹ï¸ Rate limiting already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {llm_file}: {e}")
          EOF
          
      - name: Create config
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          
          # Use single query for all databases (original working version)
          research_query = "[single-cell RNA sequencing] OR [spatial transcriptomics] OR [single-cell analysis] OR [scRNA-seq] OR [single cell atlas] OR [single-cell integration] OR [IBD] OR [inflammatory bowel disease] OR [Crohn disease] OR [ulcerative colitis] OR [mucosal immunology] OR [intestinal inflammation] OR [VEO-IBD] OR [immune-mediated inflammatory diseases] OR [primary immunodeficiencies] OR [secondary immunodeficiencies] OR [autoinflammation] OR [inflammasome biology] OR [NLRP3] OR [cytokine signaling] OR [IL-1B] OR [IL-17] OR [IL-10] OR [IL-23] OR [IL-6] OR [pyroptosis] OR [trained immunity] OR [Nod-like receptors] OR [intestinal epithelial barrier] OR [Tregs] OR [macrophages] OR [dendritic cells] OR [organoids] OR [computational biology] OR [bioinformatics] OR [multiomics] OR [proteomics] OR [machine learning] OR [data integration] OR [benchmarking] OR [computational pathology]"
          
          # Define LLM filtering prompt directly in workflow
          filtering_prompt = """You are a lab manager at a research lab focusing on single-cell RNA sequencing, spatial transcriptomics, multiomics, proteomics, machine learning applications and methods development in computational biology as well as wet lab work, organoids and perturbation. The lab also works with computational pathology, HE images, and intestinal biopsies. Lab research focuses on VEO-IBD, genetics of immune-mediated inflammatory diseases (including genetics of IBD, Crohn's disease, ulcerative colitis), primary and secondary immunodeficiencies, and autoinflammation, with translational applications of single-cell data. Key areas of interest include inflammasome biology (NLRP3, NLRP1-13, Pyrin inflammasome), metabolic pathways (MVK, PMVK), cytokine signaling (IL-1B, IL-17, IL-10, IL-23, IL-6), pyroptosis, trained immunity, Nod-like receptors, small GTPases and GEFs, intestinal epithelial barrier integrity, bacterial handling, and cellular processes including migration, EMT, and EndMT. The lab studies various immune cell types including Tregs, macrophages, antigen presenting cells, and dendritic cells. Lab members are interested in building single-cell atlases, working with single-cell data on the level of patients (donors, individuals) and keeping updated on the most recent methods in single-cell biology and organoids. Another focus of the lab is benchmarking single-cell analysis tools. A specific area of interest is single-cell data integration. You are reviewing a list of research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?"""
          
          config = {
              "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              
              # CRITICAL: Explicit database activation (like original PaperBee)
              "databases": ["pubmed", "arxiv", "biorxiv"],
              
              # Main query (used when no specific database query defined)
              "query": research_query,
              
              # Database-specific queries (optional but recommended)
              "query_biorxiv": research_query,
              "query_pubmed_arxiv": research_query,
              
              # BioRxiv-specific optimizations for stability
              "limit": 100,
              "limit_per_database": 25,  # Reduced for bioRxiv stability
              
              "LLM_FILTERING": True,
              "LLM_PROVIDER": "openai",
              "LANGUAGE_MODEL": "gemini-2.5-flash-lite",  # Upgraded to 2.5 Flash Lite
              "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
              "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
              "FILTERING_PROMPT": filtering_prompt,
              
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              },
              "TELEGRAM": {"is_posting_on": False},
              "ZULIP": {"is_posting_on": False}
          }
          
          with open("config.yml", "w") as f:
              yaml.dump(config, f, default_flow_style=False)
          
          # Debug output
          print("âœ… Generated config with explicit database activation:")
          print(f"ðŸ“Š Databases: {config['databases']}")
          print(f"ðŸ” Main query: {research_query[:100]}...")
          print(f"ðŸ§¬ BioRxiv query: {config.get('query_biorxiv', 'Using main query')[:100]}...")
          print(f"ðŸ“„ PubMed/ArXiv query: {config.get('query_pubmed_arxiv', 'Using main query')[:100]}...")
          print(f"ðŸ“Š Limits: total={config.get('limit', 'default')}, per_database={config.get('limit_per_database', 'default')}")
          print("Note: bioRxiv now explicitly activated with reduced limits for stability")
          print(f"Query contains {len(research_query.split(' OR '))} search terms")
          EOF
          
      - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "ðŸ Starting PaperBee Daily Digest"
          echo "ðŸ“… Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "ðŸ”§ NCBI timeout: 30s | Gemini 2.5 Flash Lite rate limit: 4.5s"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config.yml
