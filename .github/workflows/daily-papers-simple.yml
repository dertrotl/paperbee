# .github/workflows/daily-papers.yml
name: PaperBee Daily Digest Clinic

on:
  schedule:
    - cron: '0 7 * * 1-5'  # Monday-Friday at 7 AM UTC
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 30s)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=30/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                with open(self.search_file_biorxiv) as papers_file:
                    articles_biorxiv_dict = json.load(papers_file)["papers"]
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"Found {len(articles_pub_arx_dict)} articles from PubMed/ArXiv")
            print(f"Found {len(articles_biorxiv_dict)} articles from bioRxiv")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied bioRxiv fix to {finder_file}")
                  else:
                      print(f"â„¹ï¸ bioRxiv fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {finder_file}: {e}")
          EOF
          
          # Fix XML parsing errors in DOI extraction
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for XML error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace the problematic XML parsing line
                  old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"âš ï¸ XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied XML error handling to {utils_file}")
                  else:
                      print(f"â„¹ï¸ XML error handling already applied or pattern not found in {utils_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini rate limiting: 30 RPM = 2 seconds minimum, use 3.5s for safety
                      time.sleep(3.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(3.5)' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied Gemini rate limiting to {llm_file}")
                  else:
                      print(f"â„¹ï¸ Rate limiting already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {llm_file}: {e}")
          EOF
          
      - name: Create config
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          
          # Define separate queries for different databases
          # bioRxiv has strict requirements: only OR, no complex queries
          biorxiv_query = "[single cell] OR [cell] OR [transcriptomics] OR [omics] OR [genomics] OR [proteomics] OR [biology] OR [biomedical] OR [computational] OR [bioinformatics] OR [data science] OR [machine learning] OR [IBD] OR [bowel] OR [intestinal] OR [gut] OR [immune] OR [inflammation] OR [cytokine] OR [organoid] OR [stem cell] OR [differentiation] OR [development] OR [pathology] OR [histology] OR [microscopy] OR [imaging] OR [analysis] OR [algorithm] OR [method] OR [tool] OR [software] OR [pipeline] OR [workflow] OR [integration] OR [atlas] OR [database] OR [repository] OR [benchmark] OR [evaluation] OR [comparison] OR [spatial] OR [spatial transcriptomics]"
          
          # PubMed/ArXiv allows more complex queries
          pubmed_arxiv_query = "(single cell) OR (transcriptomics) OR (omics) OR (genomics) OR (proteomics) OR (computational biology) OR (bioinformatics) OR (machine learning) OR (deep learning) OR (artificial intelligence) OR (IBD) OR (inflammatory bowel disease) OR (bowel) OR (intestinal) OR (gut microbiome) OR (immune system) OR (inflammation) OR (cytokine) OR (T cell) OR (B cell) OR (macrophage) OR (organoid) OR (stem cell) OR (cell differentiation) OR (development) OR (pathology) OR (histology) OR (microscopy) OR (medical imaging) OR (image analysis) OR (algorithm) OR (method) OR (tool) OR (software) OR (pipeline) OR (workflow) OR (data integration) OR (atlas) OR (database) OR (repository) OR (benchmark) OR (evaluation) OR (comparison) OR (spatial transcriptomics) OR (spatial omics)"
          
          # Define LLM filtering prompt directly in workflow
          filtering_prompt = """You are a lab manager at a research lab focusing on single-cell RNA sequencing, spatial transcriptomics, multiomics, proteomics, machine learning applications and methods development in computational biology as well as wet lab work, organoids and perturbation. The lab also works with computational pathology, HE images, and intestinal biopsies. Lab research focuses on VEO-IBD, genetics of immune-mediated inflammatory diseases (including genetics of IBD, Crohn's disease, ulcerative colitis), primary and secondary immunodeficiencies, and autoinflammation, with translational applications of single-cell data. Key areas of interest include inflammasome biology (NLRP3, NLRP1-13, Pyrin inflammasome), metabolic pathways (MVK, PMVK), cytokine signaling (IL-1B, IL-17, IL-10, IL-23, IL-6), pyroptosis, trained immunity, Nod-like receptors, small GTPases and GEFs, intestinal epithelial barrier integrity, bacterial handling, and cellular processes including migration, EMT, and EndMT. The lab studies various immune cell types including Tregs, macrophages, antigen presenting cells, and dendritic cells. Lab members are interested in building single-cell atlases, working with single-cell data on the level of patients (donors, individuals) and keeping updated on the most recent methods in single-cell biology and organoids. Another focus of the lab is benchmarking single-cell analysis tools. A specific area of interest is single-cell data integration. You are reviewing a list of research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?"""
          
          config = {
              "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              
              # Use separate queries for different databases
              "query_biorxiv": biorxiv_query,
              "query_pubmed_arxiv": pubmed_arxiv_query,
              
              "LLM_FILTERING": True,
              "LLM_PROVIDER": "openai",
              "LANGUAGE_MODEL": "gemini-2.0-flash-lite",
              "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
              "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
              "FILTERING_PROMPT": filtering_prompt,
              
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              },
              "TELEGRAM": {"is_posting_on": False},
              "ZULIP": {"is_posting_on": False}
          }
          
          with open("config.yml", "w") as f:
              yaml.dump(config, f, default_flow_style=False)
          
          # Debug output
          print("Generated config:")
          print(f"bioRxiv Query: {biorxiv_query}")
          print(f"PubMed/ArXiv Query: {pubmed_arxiv_query}")
          print("Using separate optimized queries for different databases")
          print("Note: Broad queries for maximum coverage - LLM will filter for relevance")
          print(f"bioRxiv query contains {len(biorxiv_query.split(' OR '))} search terms")
          print(f"PubMed/ArXiv query contains {len(pubmed_arxiv_query.split(' OR '))} search terms")
          EOF
          
      - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "ðŸ Starting PaperBee Daily Digest"
          echo "ðŸ“… Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "ðŸ”§ NCBI timeout: 30s | Gemini rate limit: 3.5s"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config.yml
