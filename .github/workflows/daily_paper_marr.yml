# .github/workflows/daily_paper_marr.yml
name: PaperBee Daily Digest MarrPeng

on:
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6 AM UTC (8 AM German time during DST)
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        use          echo "ü©∏ Starting PaperBee Multi-Group Research Digest"
          echo "üìÖ Search window: ${              # Run PaperBee for this group
              try:
                  print(f"\nüöÄ Starting PaperBee for {group_name}...")
                  print(f"üìÖ Search window: ${{ steps.search_days.outputs.since_days }} days")
                  
                  # Set environment variable for group-specific header
                  env = os.environ.copy()
                  env["PAPERBEE_GROUP_NAME"] = group_name
                  env["PAPERBEE_GROUP_EMOJI"] = group_config["emoji"]
                  
                  result = subprocess.run([
                      "paperbee", "post", 
                      "--config", config_file, 
                      "--since", "${{ steps.search_days.outputs.since_days }}"
                  ], capture_output=True, text=True, timeout=1200, env=env)  # 20 min timeout per grouparch_days.outputs.since_days }} days"
          echo "üîß NCBI timeout: 45s | GPT-4o-mini with original rate limiting (0.2s + OpenAI delays)"
          echo "üõ†Ô∏è Enhanced error handling: NCBI connection failures, XML parsing errors"
          echo "üîß CRITICAL FIX: DOI filtering issue resolved - papers without DOI now get fallback URLs"
          echo "üí¨ Slack optimization: Multi-group consolidated messaging"
          echo "üîç Processing: 3 focused interest groups with separated BioRxiv/PubMed queries"
          echo "üìä Limits per group: 200 total, 100 per database (600 total capacity)"ns/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 45s for stability)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=45/g' {} \;
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=30/timeout=45/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue AND add debug info
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                print(f"üß¨ Searching bioRxiv with query: {self.query_biorxiv[:100]}...")
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                try:
                    with open(self.search_file_biorxiv) as papers_file:
                        articles_biorxiv_dict = json.load(papers_file)["papers"]  
                    print(f"üß¨ Found {len(articles_biorxiv_dict)} articles from bioRxiv")
                except Exception as e:
                    print(f"‚ö†Ô∏è Error reading bioRxiv results: {e}")
                    articles_biorxiv_dict = []
            else:
                print("‚ö†Ô∏è bioRxiv not in databases list")
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"üìÑ Found {len(articles_pub_arx_dict)} articles from PubMed/ArXiv")
            print(f"üß¨ Found {len(articles_biorxiv_dict)} articles from bioRxiv")
            print(f"üìä Total articles before deduplication: {len(articles_pub_arx_dict) + len(articles_biorxiv_dict)}")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied enhanced bioRxiv fix with debug info to {finder_file}")
                  else:
                      print(f"‚ÑπÔ∏è bioRxiv fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {finder_file}: {e}")
          EOF
          
          # Fix critical DOI filtering issue + comprehensive NCBI error handling
          python3 << 'EOF'
          import os
          import re
          
          # CRITICAL FIX: Patch papers_finder.py to fix URL filtering issue
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # CRITICAL: Fix the aggressive URL filtering that removes most papers
                  old_url_filter = '''        articles = [article for article in articles if article.get("url") is not None]'''
                  new_url_filter = '''        # Enhanced URL handling - don't filter out papers without URLs yet
        print(f"üìä Before URL processing: {len(articles)} articles")
        articles_with_urls = []
        articles_without_urls = []
        
        for article in articles:
            if article.get("url") is not None:
                articles_with_urls.append(article)
            else:
                # Try to create fallback URL for papers without DOI
                if article.get("title"):
                    # Create a search URL as fallback
                    search_title = article["title"].replace(" ", "+")[:100]
                    article["url"] = f"https://pubmed.ncbi.nlm.nih.gov/?term={search_title}"
                    articles_without_urls.append(article)
                    
        articles = articles_with_urls + articles_without_urls
        print(f"üìä After URL processing: {len(articles)} articles ({len(articles_with_urls)} with DOI, {len(articles_without_urls)} with fallback)")'''
                  
                  if old_url_filter in content:
                      content = content.replace(old_url_filter, new_url_filter)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ CRITICAL FIX: Applied URL filtering fix to {finder_file}")
                  else:
                      print(f"‚ÑπÔ∏è URL filtering fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {finder_file}: {e}")
          
          # Find and patch utils.py for comprehensive NCBI error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace various timeout patterns with comprehensive error handling
                  patterns_to_replace = [
                      ('fetch_response = requests.get(fetch_url, timeout=30)', 
                       '''try:
                          fetch_response = requests.get(fetch_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"‚ö†Ô∏è NCBI fetch failed: {str(e)[:50]}")
                          return None'''),
                      ('search_response = requests.get(search_url, timeout=30)',
                       '''try:
                          search_response = requests.get(search_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"‚ö†Ô∏è NCBI search failed: {str(e)[:50]}")
                          return None''')
                  ]
                  
                  changes_made = False
                  for old_pattern, new_pattern in patterns_to_replace:
                      if old_pattern in content and 'NCBI' in old_pattern and 'failed' not in content:
                          content = content.replace(old_pattern, new_pattern)
                          changes_made = True
                  
                  # Also handle XML parsing errors
                  xml_old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  xml_new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"‚ö†Ô∏è XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if xml_old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(xml_old_pattern, xml_new_pattern)
                      changes_made = True
                  
                  if changes_made:
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied comprehensive NCBI error handling to {utils_file}")
                  else:
                      print(f"‚ÑπÔ∏è NCBI error handling already applied or patterns not found in {utils_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {utils_file}: {e}")
          EOF
          
          # Comprehensive NCBI error handling with multiple timeout patterns
          python3 << 'EOF'
          import os
          import re
          
          # Additional timeout fixes for any remaining patterns
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace various timeout patterns with comprehensive error handling
                  patterns_to_replace = [
                      ('fetch_response = requests.get(ncbi_url, timeout=45)', 
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"‚ö†Ô∏è NCBI connection failed: {str(e)[:50]}")
                          return None'''),
                      ('fetch_response = requests.get(ncbi_url, timeout=30)',
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"‚ö†Ô∏è NCBI connection failed: {str(e)[:50]}")
                          return None'''),
                      ('fetch_response = requests.get(ncbi_url, timeout=10)',
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"‚ö†Ô∏è NCBI connection failed: {str(e)[:50]}")
                          return None''')
                  ]
                  
                  changes_made = False
                  for old_pattern, new_pattern in patterns_to_replace:
                      if old_pattern in content and 'NCBI connection failed' not in content:
                          content = content.replace(old_pattern, new_pattern)
                          changes_made = True
                  
                  if changes_made:
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied comprehensive NCBI error handling to {utils_file}")
                  else:
                      print(f"‚ÑπÔ∏è NCBI error handling already applied or patterns not found in {utils_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering (preserve original rate limiting)
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top if needed
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Replace the OpenAI API rate limiting for Gemini without breaking original logic
                  old_pattern = '''            # Use OpenAI API
            response = client.chat.completions.create('''
                  new_pattern = '''            # Use OpenAI API - Gemini 2.5 Flash compatible
            # Use original rate limiting: already has time.sleep(0.2) in filter_articles
            response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'Gemini 2.5 Flash compatible' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied Gemini compatibility to {llm_file} (preserved original rate limiting)")
                  else:
                      print(f"‚ÑπÔ∏è Gemini compatibility already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {llm_file}: {e}")
          EOF
          
      - name: Run Multi-Group Pipeline
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID_2 }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN_2 }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID_2 }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN_2 }}
        run: |
          echo "ü©∏ Starting PaperBee Multi-Group Research Digest"
          echo "üìÖ Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "üîß NCBI timeout: 45s | Gemini 2.5 Flash Lite rate limit: 4.5s"
          echo "üõ†Ô∏è Enhanced error handling: NCBI connection failures, XML parsing errors"
          echo "ÔøΩ CRITICAL FIX: DOI filtering issue resolved - papers without DOI now get fallback URLs"
          echo "ÔøΩüí¨ Slack optimization: Multi-group consolidated messaging"
          echo "üîç Processing: 3 focused interest groups with individual filtering"
          echo "üìä Limits per group: 200 total, 100 per database (600 total capacity)"
          echo ""
          
          python3 << 'EOF'
          import yaml
          import os
          import json
          import subprocess
          import tempfile
          from pathlib import Path
          
          # Define three focused interest groups with original PaperBee query structure
          interest_groups = {
              "Happy Pixels": {
                  "emoji": "üñºÔ∏è",
                  "query_biorxiv": "[computer vision] OR [image analysis] OR [image processing] OR [image segmentation] OR [cell segmentation] OR [3D segmentation] OR [microscopy] OR [fluorescence microscopy] OR [quantitative microscopy] OR [AI microscopy] OR [biomedical image analysis] OR [medical imaging] OR [deep learning imaging] OR [machine learning microscopy] OR [foundation models imaging] OR [self-supervised learning images] OR [cell classification] OR [cell detection] OR [organoid detection] OR [organoid segmentation] OR [holography] OR [digital holography]",
                  "query_pubmed_arxiv": "([computer vision] OR [image analysis] OR [image processing] OR [image segmentation]) AND ([microscopy] OR [medical imaging] OR [biomedical imaging]) AND NOT ([proteomics] OR [genomics without imaging])",
                  "filter": "You are a lab manager at a research lab focusing on computational imaging and computer vision in biomedical contexts. Lab members are interested in image analysis algorithms, computer vision methods for microscopy, and AI/ML approaches for biological image processing. You are reviewing research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?",
                  "limit": 400,
                  "limit_per_database": 150
              },
              "Omics and Dynamics": {
                  "emoji": "üß¨",
                  "query_biorxiv": "[single cell] OR [single-cell] OR [multi-omics] OR [multiomics] OR [spatial transcriptomics] OR [mathematical modeling] OR [mechanistic modeling] OR [clonal hematopoiesis] OR [CHIP] OR [clonal dynamics] OR [hematopoiesis] OR [computational biology dynamics] OR [systems biology] OR [mathematical biology] OR [modeling hematopoiesis] OR [omics computational] OR [transcriptomics analysis]",
                  "query_pubmed_arxiv": "([single-cell transcriptomics] OR [spatial transcriptomics] OR [multi-omics] OR [computational biology]) AND ([dynamics] OR [modeling] OR [hematopoiesis] OR [clonal evolution]) AND NOT ([proteomics without transcriptomics] OR [drug discovery])",
                  "filter": "You are a lab manager at a research lab focusing on computational analysis of biological systems and dynamics. Lab members are interested in single-cell omics analysis, mathematical modeling of biological systems, and clonal hematopoiesis research. You are reviewing research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?",
                  "limit": 400,
                  "limit_per_database": 150
              },
              "Histopathology": {
                  "emoji": "üî¨",
                  "query_biorxiv": "[computational pathology] OR [digital pathology] OR [pathology AI] OR [histopathology] OR [hematopathology] OR [pathology machine learning] OR [pathology deep learning] OR [morphological profiling] OR [cell painting] OR [representation learning pathology] OR [blood disorders computational] OR [hematology AI] OR [leukemia computational] OR [lymphoma computational] OR [myeloma computational]",
                  "query_pubmed_arxiv": "([computational pathology] OR [digital pathology] OR [pathology]) AND ([AI] OR [machine learning] OR [deep learning] OR [computational analysis]) AND ([hematology] OR [blood disorders] OR [histopathology]) AND NOT ([pure clinical studies] OR [case reports without AI])",
                  "filter": "You are a lab manager at a research lab focusing on computational pathology and blood disorders. Lab members are interested in AI/ML approaches for histopathological analysis, hematopathology with computational components, and digital pathology methods. You are reviewing research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?",
                  "limit": 400,
                  "limit_per_database": 150
              }
          }
          
          all_papers = []
          
              print(f"üéØ PROCESSING OVERVIEW:")
          for i, (group_name, group_config) in enumerate(interest_groups.items(), 1):
              biorxiv_terms = len(group_config['query_biorxiv'].split(' OR '))
              pubmed_terms = len(group_config['query_pubmed_arxiv'].split(' OR '))
              print(f"  {i}. {group_config['emoji']} {group_name}: {biorxiv_terms} bioRxiv terms, {pubmed_terms} PubMed/ArXiv terms")
          print("")
          
          for group_name, group_config in interest_groups.items():
              print(f"\n{group_config['emoji']} Processing {group_name} group...")
              
              # Create config for this group using original PaperBee structure
              config = {
                  "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
                  "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
                  "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
                  "LOCAL_ROOT_DIR": ".",
                  "databases": ["pubmed", "arxiv", "biorxiv"],
                  "query_biorxiv": group_config["query_biorxiv"],
                  "query_pubmed_arxiv": group_config["query_pubmed_arxiv"],
                  "limit": group_config["limit"],
                  "limit_per_database": group_config["limit_per_database"],
                  "LLM_FILTERING": True,
                  "LLM_PROVIDER": "openai",
                  "LANGUAGE_MODEL": "gpt-4o-mini",
                  "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
                  "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
                  "FILTERING_PROMPT": group_config["filter"],
                  "SLACK": {
                      "is_posting_on": True,
                      "bot_token": os.environ.get("SLACK_BOT", ""),
                      "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                      "app_token": os.environ.get("SLACK_APP", "")
                  },
                  "TELEGRAM": {"is_posting_on": False},
                  "ZULIP": {"is_posting_on": False}
              }              # Create temporary config file
              config_file = f"config_{group_name.lower().replace(' ', '_')}.yml"
              with open(config_file, "w") as f:
                  yaml.dump(config, f, default_flow_style=False)
              
              print(f"üìã Config for {group_name}:")
              print(f"   üß¨ BioRxiv query: {group_config['query_biorxiv'][:100]}...")
              print(f"   ÔøΩ PubMed/ArXiv query: {group_config['query_pubmed_arxiv'][:100]}...")
              print(f"   üìä Limits: total={group_config['limit']}, per_db={group_config['limit_per_database']}")
              print(f"   üóÉÔ∏è Databases: {config['databases']}")
              print(f"   ü§ñ LLM: {config['LANGUAGE_MODEL']} with simplified filtering")
              
              # Run PaperBee for this group
              try:
                  print(f"\nÔøΩ Starting PaperBee for {group_name}...")
                  print(f"üìÖ Search window: ${{ steps.search_days.outputs.since_days }} days")
                  
                  result = subprocess.run([
                      "paperbee", "post", 
                      "--config", config_file, 
                      "--since", "${{ steps.search_days.outputs.since_days }}"
                  ], capture_output=True, text=True, timeout=1200)  # 20 min timeout per group
                  
                  print(f"\nüì§ PaperBee output for {group_name}:")
                  print("="*60)
                  print(result.stdout)
                  if result.stderr:
                      print(f"‚ö†Ô∏è Stderr: {result.stderr}")
                  print("="*60)
                  
                  if result.returncode == 0:
                      print(f"‚úÖ {group_name} completed successfully")
                      # Try to read results from output
                      output_lines = result.stdout.split('\n')
                      paper_count = 0
                      raw_count = 0
                      
                      # Parse various output patterns
                      for line in output_lines:
                          if 'papers selected after filtering' in line.lower():
                              try:
                                  paper_count = int(line.split()[0])
                              except:
                                  pass
                          elif 'found' in line.lower() and 'articles' in line.lower():
                              try:
                                  numbers = [int(s) for s in line.split() if s.isdigit()]
                                  if numbers:
                                      raw_count += numbers[0]
                              except:
                                  pass
                      
                      print(f"üìà Results for {group_name}: {raw_count} raw ‚Üí {paper_count} filtered")
                      
                      all_papers.append({
                          "group": group_name,
                          "emoji": group_config["emoji"],
                          "count": paper_count,
                          "raw_count": raw_count,
                          "status": "success"
                      })
                  else:
                      print(f"‚ùå {group_name} failed with return code {result.returncode}")
                      print(f"Error details: {result.stderr}")
                      all_papers.append({
                          "group": group_name,
                          "emoji": group_config["emoji"],
                          "count": 0,
                          "raw_count": 0,
                          "status": "failed"
                      })
                      
              except subprocess.TimeoutExpired:
                  print(f"‚è∞ {group_name} timed out after 20 minutes")
                  all_papers.append({
                      "group": group_name,
                      "emoji": group_config["emoji"],
                      "count": 0,
                      "raw_count": 0,
                      "status": "timeout"
                  })
              except Exception as e:
                  print(f"üí• {group_name} error: {str(e)}")
                  all_papers.append({
                      "group": group_name,
                      "emoji": group_config["emoji"],
                      "count": 0,
                      "raw_count": 0,
                      "status": "error"
                  })
              
              # Clean up config file
              try:
                  os.remove(config_file)
              except:
                  pass
          
          # Final summary for console only
          print(f"\nüìã FINAL SUMMARY:")
          total_papers = sum(p["count"] for p in all_papers if p["status"] == "success")
          total_raw = sum(p.get("raw_count", 0) for p in all_papers if p["status"] == "success")
          print(f"üî¢ Total papers: {total_raw} raw ‚Üí {total_papers} filtered")
          for paper_info in all_papers:
              raw_info = f" ({paper_info.get('raw_count', 0)} raw)" if paper_info.get('raw_count', 0) > 0 else ""
              print(f"  {paper_info['emoji']} {paper_info['group']}: {paper_info['count']} papers{raw_info} ({paper_info['status']})")
          
          print(f"\nüìù Note: Each group posted their papers separately to Slack")
          
          EOF
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config_*.yml
