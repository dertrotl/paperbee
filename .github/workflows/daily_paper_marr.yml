# .github/workflows/daily_paper_marr.yml
name: PaperBee Daily Digest MarrPeng

on:
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6 AM UTC (8 AM German time during DST)
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
          - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "ðŸ©¸ Starting PaperBee Research Digest"
          echo "ðŸ“… Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "ðŸ”§ NCBI timeout: 45s | Gemini 2.5 Flash Lite rate limit: 4.5s"
          echo "ðŸ› ï¸ Enhanced error handling: NCBI connection failures, XML parsing errors"
          echo "ðŸ’¬ Slack optimization: Batched papers to prevent 50-block limit errors"
          echo "ðŸ” Filtering: Selective computational biomedical focus (pathology, microscopy, segmentation)"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 45s for stability)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=45/g' {} \;
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=30/timeout=45/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                with open(self.search_file_biorxiv) as papers_file:
                    articles_biorxiv_dict = json.load(papers_file)["papers"]
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"Found {len(articles_pub_arx_dict)} articles from PubMed/ArXiv")
            print(f"Found {len(articles_biorxiv_dict)} articles from bioRxiv")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied bioRxiv fix to {finder_file}")
                  else:
                      print(f"â„¹ï¸ bioRxiv fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {finder_file}: {e}")
          EOF
          
          # Comprehensive NCBI error handling with multiple timeout patterns
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for comprehensive NCBI error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace various timeout patterns with comprehensive error handling
                  patterns_to_replace = [
                      ('fetch_response = requests.get(ncbi_url, timeout=45)', 
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"âš ï¸ NCBI connection failed: {str(e)[:50]}")
                          return None'''),
                      ('fetch_response = requests.get(ncbi_url, timeout=30)',
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"âš ï¸ NCBI connection failed: {str(e)[:50]}")
                          return None'''),
                      ('fetch_response = requests.get(ncbi_url, timeout=10)',
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"âš ï¸ NCBI connection failed: {str(e)[:50]}")
                          return None''')
                  ]
                  
                  changes_made = False
                  for old_pattern, new_pattern in patterns_to_replace:
                      if old_pattern in content and 'NCBI connection failed' not in content:
                          content = content.replace(old_pattern, new_pattern)
                          changes_made = True
                  
                  # Also handle XML parsing errors
                  xml_old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  xml_new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"âš ï¸ XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if xml_old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(xml_old_pattern, xml_new_pattern)
                      changes_made = True
                  
                  if changes_made:
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied comprehensive NCBI error handling to {utils_file}")
                  else:
                      print(f"â„¹ï¸ NCBI error handling already applied or patterns not found in {utils_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini 2.5 Flash Lite rate limiting: 15 RPM = 4s minimum, use 4.5s for safety
                      time.sleep(4.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied Gemini 2.5 Flash Lite rate limiting to {llm_file}")
                  else:
                      print(f"â„¹ï¸ Rate limiting already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {llm_file}: {e}")
          EOF
          
      - name: Create config
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID_2 }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN_2 }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID_2 }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN_2 }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          
          # Use single query for all databases (original working version)
          research_query = "[hematopoiesis] OR [haematopoiesis] OR [hematology] OR [haematology] OR [blood disorders] OR [computational pathology] OR [cell segmentation] OR [segmentation] OR [3D segmentation] OR [single cell] OR [single-cell] OR [machine learning] OR [deep learning] OR [artificial intelligence] OR [computer vision] OR [image analysis] OR [microscopy] OR [computational biology] OR [multi-omics] OR [multiomics] OR [foundation models] OR [self-supervised learning] OR [cell classification] OR [cell detection] OR [image processing] OR [biomedical image analysis] OR [medical imaging] OR [AI microscopy] OR [quantitative microscopy] OR [fluorescence microscopy] OR [vision language model] OR [large language models] OR [mathematical modeling] OR [mechanistic modeling] OR [drug target identification] OR [spatial transcriptomics] OR [pathology] OR [hematopathology] OR [blood cell] OR [leukemia] OR [lymphoma] OR [myeloma] OR [CHIP] OR [clonal hematopoiesis] OR [LLM] OR [language model] OR [morphological profiling] OR [cell painting] OR [representation learning] OR [organoid detection] OR [organoid segmentation] OR [holography] OR [digital in-line holography]"
          
          # Define LLM filtering prompt directly in workflow - SELECTIVE FILTERING
          filtering_prompt = """You are a selective lab manager at a computational biomedical research lab. You must be STRICT but not overly narrow in selecting relevant papers. The lab focuses on:

CORE RESEARCH AREAS (prioritize these):
1. Computational hematology and blood disorders with ML/AI approaches
2. Computational pathology (digital pathology, pathology AI, histopathology analysis)
3. Cell segmentation and classification (any biological context with computational methods)
4. Computer vision and image analysis for biomedical applications
5. Machine learning for microscopy image analysis (any biological samples)
6. Single-cell analysis with computational methods (preferably but not exclusively hematopoiesis)
7. Clonal hematopoiesis (CHIP) and clonal dynamics with computational methods
8. Mechanistic mathematical modeling of biological systems
9. Foundation models and self-supervised learning for medical/biological imaging
10. Multi-omics approaches with computational analysis
11. AI-based algorithms for medical diagnosis and biological discovery
12. Morphological profiling and representation learning for cellular phenotypes
13. Organoid detection, segmentation, and analysis
14. Advanced microscopy techniques with computational components

STRICT EXCLUSION CRITERIA - IMMEDIATELY REJECT if paper is about:
- Pure clinical studies without computational methodology
- Drug discovery without computational modeling or AI components
- General AI/ML methodology papers without clear biomedical application
- Social sciences, economics, or non-biomedical applications
- Pure wet-lab methodology without computational analysis
- Infectious disease studies without computational modeling aspects
- General bioinformatics without novel computational methodology

DECISION RULES:
- Paper must involve computational methods, AI/ML, or advanced imaging analysis
- Must have clear biomedical or biological application
- Computational pathology, microscopy, and segmentation papers are highly relevant
- Hematology focus is preferred but not required if computationally interesting
- When in doubt about computational relevance, lean towards 'yes'
- Aim to select maximum 35-45 high-quality papers per day

You are reviewing research papers for daily digest. Be selective but not overly restrictive - papers with strong computational methodology for biomedical applications should pass.

Answer ONLY 'yes' or 'no': Is this paper relevant for our computational biomedical research with focus on pathology, microscopy, and biological image analysis?"""
          
          config = {
              "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              
              # CRITICAL: Explicit database activation (like original PaperBee)
              "databases": ["pubmed", "arxiv", "biorxiv"],
              
              # Main query (used when no specific database query defined)
              "query": research_query,
              
              # Database-specific queries (optional but recommended)
              "query_biorxiv": research_query,
              "query_pubmed_arxiv": research_query,
              
              # BioRxiv-specific optimizations for stability
              "limit": 100,
              "limit_per_database": 25,  # Reduced for bioRxiv stability
              
              "LLM_FILTERING": True,
              "LLM_PROVIDER": "openai",
              "LANGUAGE_MODEL": "gemini-2.5-flash-lite",  # Upgraded to 2.5 Flash Lite
              "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
              "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
              "FILTERING_PROMPT": filtering_prompt,
              
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              },
              "TELEGRAM": {"is_posting_on": False},
              "ZULIP": {"is_posting_on": False}
          }
          
          with open("config.yml", "w") as f:
              yaml.dump(config, f, default_flow_style=False)
          
          # Debug output
          print("âœ… Generated config with explicit database activation:")
          print(f"ðŸ“Š Databases: {config['databases']}")
          print(f"ðŸ” Main query: {research_query[:100]}...")
          print(f"ðŸ§¬ BioRxiv query: {config.get('query_biorxiv', 'Using main query')[:100]}...")
          print(f"ðŸ“„ PubMed/ArXiv query: {config.get('query_pubmed_arxiv', 'Using main query')[:100]}...")
          print(f"ðŸ“Š Limits: total={config.get('limit', 'default')}, per_database={config.get('limit_per_database', 'default')}")
          print("Note: bioRxiv now explicitly activated with reduced limits for stability")
          print(f"Query contains {len(research_query.split(' OR '))} search terms")
          EOF
          
      - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "ï¿½ Starting PaperBee Research Digest"
          echo "ðŸ“… Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "ðŸ”§ NCBI timeout: 30s | Gemini 2.5 Flash Lite rate limit: 4.5s"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config.yml
