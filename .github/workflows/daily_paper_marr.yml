# .github/workflows/daily_paper_marr.yml
name: PaperBee Daily Digest MarrPeng

on:
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6 AM UTC (8 AM German time during DST)
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Run Multi-Group Pipeline
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID_2 }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN_2 }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID_2 }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN_2 }}
        run: |
          echo "üî¨ Starting PaperBee Multi-Group Research Digest"
          echo "üìÖ Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo ""
          
          python3 << 'EOF'
          import yaml
          import os
          import json
          import subprocess
          import tempfile
          from pathlib import Path
          
          # Define three focused interest groups for MarrLab
          interest_groups = {
              "Happy Pixels": {
                  "emoji": "üñºÔ∏è",
                  "query_biorxiv": "[computational pathology] OR [machine learning pathology] OR [deep learning medical] OR [AI pathology] OR [computer vision biomedical] OR [image analysis biomedical] OR [cell segmentation] OR [foundation models] OR [vision language model] OR [multimodal AI] OR [computational microscopy] OR [automated cell analysis]",
                  "query_pubmed_arxiv": "[computational pathology] OR [machine learning pathology] OR [deep learning medical imaging] OR [AI biomedical imaging] OR [computer vision medical] OR [automated image analysis] OR [foundation models medical] OR [vision language model] OR [multimodal AI medical] OR [computational microscopy]",
                  "filter": "You are reviewing papers for a lab focused on computational pathology, microscopy image analysis, and AI/ML for biomedical imaging. Remember: we are primarily interested in COMPUTATIONAL papers. Key interests: cell segmentation, image processing, foundation models, vision language models, multimodal AI, and computer vision in life sciences. Be INCLUSIVE - if a paper relates to image analysis, microscopy, AI/ML in biomedical contexts, vision language models, or computational imaging, it is likely relevant. Prioritize computational/algorithmic approaches. Answer 'yes' or 'no': Is this paper relevant?",
                  "limit": 400,
                  "limit_per_database": 150
              },
              "Omics and Dynamics": {
                  "emoji": "üß¨",
                  "query_biorxiv": "[computational biology hematology] OR [computational hematology] OR [mathematical modeling hematology] OR [systems biology blood] OR [bioinformatics hematology] OR [single cell computational] OR [machine learning hematology] OR [quantitative biology blood] OR [clonal dynamics modeling] OR [computational omics]",
                  "query_pubmed_arxiv": "[computational hematology] OR [mathematical modeling hematology] OR [systems biology blood] OR [bioinformatics hematology] OR [machine learning hematology] OR [quantitative hematology] OR [computational biology blood disorders] OR [algorithmic hematology] OR [statistical modeling blood]",
                  "filter": "You are reviewing papers for a lab focused on computational hematology, blood disorders, and quantitative biology. Remember: we are primarily interested in COMPUTATIONAL papers. Key interests: hematopoiesis, clonal dynamics, mathematical modeling, and multi-omics approaches. Be INCLUSIVE - if a paper relates to hematology, blood disorders, single-cell analysis, computational biology, or quantitative biology, it is likely relevant. Prioritize computational/quantitative approaches. Answer 'yes' or 'no': Is this paper relevant?",
                  "limit": 400,
                  "limit_per_database": 150
              },
              "Histopathology": {
                  "emoji": "üî¨",
                  "query_biorxiv": "[computational pathology] OR [AI pathology] OR [machine learning pathology] OR [deep learning pathology] OR [computer aided diagnosis] OR [automated pathology] OR [digital pathology AI] OR [computational hematopathology] OR [AI medical diagnosis] OR [machine learning medical]",
                  "query_pubmed_arxiv": "[computational pathology] OR [AI pathology] OR [machine learning pathology] OR [deep learning medical diagnosis] OR [computer aided pathology] OR [automated diagnosis] OR [AI hematopathology] OR [computational medical diagnosis] OR [algorithmic pathology]",
                  "filter": "You are reviewing papers for a lab focused on computational pathology, hematopathology, and medical AI. Remember: we are primarily interested in COMPUTATIONAL papers. Key interests: hematology, blood disorders, computational pathology, and AI-based diagnostics. Be INCLUSIVE - if a paper relates to hematology, pathology, medical AI, or computational medicine, it is likely relevant. Prioritize computational/AI approaches. Answer 'yes' or 'no': Is this paper relevant?",,
                  "limit": 400,
                  "limit_per_database": 150
              }
          }
          
          all_papers = []
          
          print(f"üéØ PROCESSING OVERVIEW:")
          for i, (group_name, group_config) in enumerate(interest_groups.items(), 1):
              biorxiv_terms = len(group_config['query_biorxiv'].split(' OR '))
              pubmed_terms = len(group_config['query_pubmed_arxiv'].split(' OR '))
              print(f"  {i}. {group_config['emoji']} {group_name}: {biorxiv_terms} bioRxiv terms, {pubmed_terms} PubMed/ArXiv terms")
          print("")
          
          for group_name, group_config in interest_groups.items():
              print(f"\n{group_config['emoji']} Processing {group_name} group...")
              
              # Create config for this group
              config = {
                  "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              "databases": ["pubmed", "biorxiv"],
                  "query_biorxiv": group_config["query_biorxiv"],
                  "query_pubmed_arxiv": group_config["query_pubmed_arxiv"],
                  "limit": group_config["limit"],
                  "limit_per_database": group_config["limit_per_database"],
                  "LLM_FILTERING": True,
                  "LLM_PROVIDER": "openai",
                  "LANGUAGE_MODEL": "gemini-2.5-flash-lite",
                  "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
                  "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
                  "FILTERING_PROMPT": group_config["filter"],
                  "SLACK": {
                      "is_posting_on": True,
                      "bot_token": os.environ.get("SLACK_BOT", ""),
                      "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                      "app_token": os.environ.get("SLACK_APP", "")
                  },
                  "TELEGRAM": {"is_posting_on": False},
                  "ZULIP": {"is_posting_on": False}
              }
              
              # Create temporary config file
              config_file = f"config_{group_name.lower().replace(' ', '_')}.yml"
              with open(config_file, "w") as f:
                  yaml.dump(config, f, default_flow_style=False)
              
              print(f"üìã Config for {group_name}:")
              print(f"   üß¨ BioRxiv query: {group_config['query_biorxiv'][:100]}...")
              print(f"   üìÑ PubMed/ArXiv query: {group_config['query_pubmed_arxiv'][:100]}...")
              print(f"   üìä Limits: total={group_config['limit']}, per_db={group_config['limit_per_database']}")
              print(f"   üóÉÔ∏è Databases: {config['databases']}")
              print(f"   ü§ñ LLM: {config['LANGUAGE_MODEL']}")
              print(f"   üîç DEBUG - Full config databases value: {repr(config['databases'])}")
              print(f"   üîç DEBUG - Type of databases: {type(config['databases'])}")
              
              # Run PaperBee for this group
              try:
                  print(f"\nüöÄ Starting PaperBee for {group_name}...")
                  print(f"üìÖ Search window: ${{ steps.search_days.outputs.since_days }} days")
                  
                  # Set environment variable for group-specific header
                  env = os.environ.copy()
                  env["PAPERBEE_GROUP_NAME"] = group_name
                  env["PAPERBEE_GROUP_EMOJI"] = group_config["emoji"]
                  
                  result = subprocess.run([
                      "paperbee", "post", 
                      "--config", config_file, 
                      "--since", "${{ steps.search_days.outputs.since_days }}"
                  ], capture_output=True, text=True, timeout=1200, env=env)  # 20 min timeout per group
                  
                  print(f"\nüì§ PaperBee output for {group_name}:")
                  print("="*60)
                  print(result.stdout)
                  if result.stderr:
                      print(f"‚ö†Ô∏è Stderr: {result.stderr}")
                  print("="*60)
                  
                  if result.returncode == 0:
                      print(f"‚úÖ {group_name} completed successfully")
                      # Try to read results from output
                      output_lines = result.stdout.split('\n')
                      paper_count = 0
                      raw_count = 0
                      
                      # Parse various output patterns
                      for line in output_lines:
                          if 'papers selected after filtering' in line.lower():
                              try:
                                  paper_count = int(line.split()[0])
                              except:
                                  pass
                          elif 'found' in line.lower() and 'articles' in line.lower():
                              try:
                                  numbers = [int(s) for s in line.split() if s.isdigit()]
                                  if numbers:
                                      raw_count += numbers[0]
                              except:
                                  pass
                      
                      print(f"üìà Results for {group_name}: {raw_count} raw ‚Üí {paper_count} filtered")
                      
                      all_papers.append({
                          "group": group_name,
                          "emoji": group_config["emoji"],
                          "count": paper_count,
                          "raw_count": raw_count,
                          "status": "success"
                      })
                  else:
                      print(f"‚ùå {group_name} failed with return code {result.returncode}")
                      print(f"Error details: {result.stderr}")
                      all_papers.append({
                          "group": group_name,
                          "emoji": group_config["emoji"],
                          "count": 0,
                          "raw_count": 0,
                          "status": "failed"
                      })
                      
              except subprocess.TimeoutExpired:
                  print(f"‚è∞ {group_name} timed out after 20 minutes")
                  all_papers.append({
                      "group": group_name,
                      "emoji": group_config["emoji"],
                      "count": 0,
                      "raw_count": 0,
                      "status": "timeout"
                  })
              except Exception as e:
                  print(f"üí• {group_name} error: {str(e)}")
                  all_papers.append({
                      "group": group_name,
                      "emoji": group_config["emoji"],
                      "count": 0,
                      "raw_count": 0,
                      "status": "error"
                  })
              
              # Clean up config file
              try:
                  os.remove(config_file)
              except:
                  pass
          
          # Final summary for console only
          print(f"\nüìã FINAL SUMMARY:")
          total_papers = sum(p["count"] for p in all_papers if p["status"] == "success")
          total_raw = sum(p.get("raw_count", 0) for p in all_papers if p["status"] == "success")
          print(f"üî¢ Total papers: {total_raw} raw ‚Üí {total_papers} filtered")
          for paper_info in all_papers:
              raw_info = f" ({paper_info.get('raw_count', 0)} raw)" if paper_info.get('raw_count', 0) > 0 else ""
              print(f"  {paper_info['emoji']} {paper_info['group']}: {paper_info['count']} papers{raw_info} ({paper_info['status']})")
          
          print(f"\nüìù Note: Each group posted their papers separately to Slack")
          
          EOF
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config_*.yml
