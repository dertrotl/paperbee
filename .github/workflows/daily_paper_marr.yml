# .github/workflows/daily_paper_marr.yml
name: PaperBee Daily Digest MarrPeng

on:
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6 AM UTC (8 AM German time during DST)
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 30s)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=30/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                with open(self.search_file_biorxiv) as papers_file:
                    articles_biorxiv_dict = json.load(papers_file)["papers"]
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"Found {len(articles_pub_arx_dict)} articles from PubMed/ArXiv")
            print(f"Found {len(articles_biorxiv_dict)} articles from bioRxiv")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"✅ Applied bioRxiv fix to {finder_file}")
                  else:
                      print(f"ℹ️ bioRxiv fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"⚠️ Error processing {finder_file}: {e}")
          EOF
          
          # Fix XML parsing errors in DOI extraction
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for comprehensive error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace the problematic NCBI request with comprehensive error handling
                  old_pattern = 'fetch_response = requests.get(fetch_url, timeout=30)  # Added timeout'
                  new_pattern = '''try:
                      fetch_response = requests.get(fetch_url, timeout=45)  # Increased timeout
                  except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, 
                          requests.exceptions.RequestException) as e:
                      print(f"⚠️ NCBI connection failed, skipping DOI extraction: {str(e)[:100]}")
                      return None'''
                  
                  if old_pattern in content and 'NCBI connection failed' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      # Also fix XML parsing
                      xml_old = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                      xml_new = '''try:
                          root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                      except (ET.ParseError, Exception) as e:
                          print(f"⚠️ XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                          return None'''
                      
                      if xml_old in content and 'XML parsing failed' not in content:
                          content = content.replace(xml_old, xml_new)
                      
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"✅ Applied comprehensive NCBI error handling to {utils_file}")
                  else:
                      print(f"ℹ️ NCBI error handling already applied or pattern not found in {utils_file}")
                      
              except Exception as e:
                  print(f"⚠️ Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini 2.5 Flash Lite rate limiting: 15 RPM = 4s minimum, use 4.5s for safety
                      time.sleep(4.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"✅ Applied Gemini 2.5 Flash Lite rate limiting to {llm_file}")
                  else:
                      print(f"ℹ️ Rate limiting already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"⚠️ Error processing {llm_file}: {e}")
          EOF
          
      - name: Create config
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID_2 }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN_2 }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID_2 }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN_2 }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          
          # Use single query for all databases (original working version)
          research_query = "[hematopoiesis] OR [haematopoiesis] OR [hematology] OR [haematology] OR [blood disorders] OR [computational pathology] OR [cell segmentation] OR [segmentation] OR [3D segmentation] OR [single cell] OR [single-cell] OR [machine learning] OR [deep learning] OR [artificial intelligence] OR [computer vision] OR [image analysis] OR [microscopy] OR [computational biology] OR [multi-omics] OR [multiomics] OR [foundation models] OR [self-supervised learning] OR [cell classification] OR [cell detection] OR [image processing] OR [biomedical image analysis] OR [medical imaging] OR [AI microscopy] OR [quantitative microscopy] OR [fluorescence microscopy] OR [vision language model] OR [large language models] OR [mathematical modeling] OR [mechanistic modeling] OR [drug target identification] OR [spatial transcriptomics] OR [pathology] OR [hematopathology] OR [blood cell] OR [leukemia] OR [lymphoma] OR [myeloma] OR [CHIP] OR [clonal hematopoiesis] OR [LLM] OR [language model] OR [morphological profiling] OR [cell painting] OR [representation learning] OR [organoid detection] OR [organoid segmentation] OR [holography] OR [digital in-line holography]"
          
          # Define LLM filtering prompt directly in workflow
          filtering_prompt = """You are a lab manager at a research lab focusing on computational hematology, blood disorders, and microscopy image analysis, computational pathology and bioinformatics. The lab develops machine learning algorithms for cell segmentation, computational pathology, and single-cell profiling in hematological contexts. Key research areas include clonal hematopoiesis (CHIP), clonal dynamics, mechanistic mathematical modeling of haematopoiesis, and multi-omics approaches to blood cell production and disorders. The lab works extensively with foundation models, self-supervised learning, and multiple instance learning for analyzing microscopic images including bright-field, fluorescence, cryo-electron, and extended depth-of-field microscopy. Research focuses on developing AI-based algorithms for cell detection, classification, and quantification to advance diagnosis and treatment of severe blood disorders. Lab members are interested in computational hematopathology, single cell-based big data analysis for drug target identification, mechanistic modeling of time-resolved hematological data, and large language models/agents in healthcare applications. The lab combines AI with biomedical knowledge to classify individual cells and patients, with particular emphasis on quantitative microscopy image processing and custom machine learning methods for life scientists and pathologists. Additionally, the lab works on morphological profiling, cell painting assays, representation learning for cellular phenotypes, organoid detection and segmentation, holography, and digital in-line holography for advanced microscopy applications. You should be INCLUSIVE rather than exclusive in your filtering. Papers that are even tangentially related to machine learning, computational biology, single-cell analysis, hematology, pathology, image analysis, morphological profiling, cell painting, organoid research, or holographic microscopy should be considered relevant. When in doubt, choose 'yes' rather than 'no'. You are reviewing a list of research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?"""
          
          config = {
              "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              
              # CRITICAL: Explicit database activation (like original PaperBee)
              "databases": ["pubmed", "arxiv", "biorxiv"],
              
              # Main query (used when no specific database query defined)
              "query": research_query,
              
              # Database-specific queries (optional but recommended)
              "query_biorxiv": research_query,
              "query_pubmed_arxiv": research_query,
              
              # BioRxiv-specific optimizations for stability
              "limit": 100,
              "limit_per_database": 25,  # Reduced for bioRxiv stability
              
              "LLM_FILTERING": True,
              "LLM_PROVIDER": "openai",
              "LANGUAGE_MODEL": "gemini-2.5-flash-lite",  # Upgraded to 2.5 Flash Lite
              "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
              "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
              "FILTERING_PROMPT": filtering_prompt,
              
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              },
              "TELEGRAM": {"is_posting_on": False},
              "ZULIP": {"is_posting_on": False}
          }
          
          with open("config.yml", "w") as f:
              yaml.dump(config, f, default_flow_style=False)
          
          # Debug output
          print("✅ Generated config with explicit database activation:")
          print(f"📊 Databases: {config['databases']}")
          print(f"🔍 Main query: {research_query[:100]}...")
          print(f"🧬 BioRxiv query: {config.get('query_biorxiv', 'Using main query')[:100]}...")
          print(f"📄 PubMed/ArXiv query: {config.get('query_pubmed_arxiv', 'Using main query')[:100]}...")
          print(f"📊 Limits: total={config.get('limit', 'default')}, per_database={config.get('limit_per_database', 'default')}")
          print("Note: bioRxiv now explicitly activated with reduced limits for stability")
          print(f"Query contains {len(research_query.split(' OR '))} search terms")
          EOF
          
      - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "� Starting PaperBee Research Digest"
          echo "📅 Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "🔧 NCBI timeout: 45s | Gemini 2.5 Flash Lite rate limit: 4.5s"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config.yml
