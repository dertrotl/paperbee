# .github/workflows/daily-papers-simple.yml
name: PaperBee Daily Digest Clinic

on:
  schedule:
    - cron: '0 7 * * 1-5'  # Monday-Friday at 7 AM UTC
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 45s for stability)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=45/g' {} \;
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=30/timeout=45/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                with open(self.search_file_biorxiv) as papers_file:
                    articles_biorxiv_dict = json.load(papers_file)["papers"]
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"Found {len(articles_pub_arx_dict)} articles from PubMed/ArXiv")
            print(f"Found {len(articles_biorxiv_dict)} articles from bioRxiv")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied bioRxiv fix to {finder_file}")
                  else:
                      print(f"â„¹ï¸ bioRxiv fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {finder_file}: {e}")
          EOF
          
          # Comprehensive NCBI error handling with multiple timeout patterns
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for comprehensive NCBI error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace various timeout patterns with comprehensive error handling
                  patterns_to_replace = [
                      ('fetch_response = requests.get(ncbi_url, timeout=45)', 
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"âš ï¸ NCBI connection failed: {str(e)[:50]}")
                          return None'''),
                      ('fetch_response = requests.get(ncbi_url, timeout=30)',
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"âš ï¸ NCBI connection failed: {str(e)[:50]}")
                          return None'''),
                      ('fetch_response = requests.get(ncbi_url, timeout=10)',
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"âš ï¸ NCBI connection failed: {str(e)[:50]}")
                          return None''')
                  ]
                  
                  changes_made = False
                  for old_pattern, new_pattern in patterns_to_replace:
                      if old_pattern in content and 'NCBI connection failed' not in content:
                          content = content.replace(old_pattern, new_pattern)
                          changes_made = True
                  
                  # Also handle XML parsing errors
                  xml_old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  xml_new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"âš ï¸ XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if xml_old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(xml_old_pattern, xml_new_pattern)
                      changes_made = True
                  
                  if changes_made:
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied comprehensive NCBI error handling to {utils_file}")
                  else:
                      print(f"â„¹ï¸ NCBI error handling already applied or patterns not found in {utils_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini 2.5 Flash Lite rate limiting: 15 RPM = 4s minimum, use 4.5s for safety
                      time.sleep(4.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"âœ… Applied Gemini 2.5 Flash Lite rate limiting to {llm_file}")
                  else:
                      print(f"â„¹ï¸ Rate limiting already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"âš ï¸ Error processing {llm_file}: {e}")
          EOF
          
      - name: Create config
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          
          # Use single query for all databases (original working version)
          research_query = "[single-cell RNA sequencing] OR [spatial transcriptomics] OR [single-cell analysis] OR [scRNA-seq] OR [single cell atlas] OR [single-cell integration] OR [IBD] OR [inflammatory bowel disease] OR [Crohn disease] OR [ulcerative colitis] OR [mucosal immunology] OR [intestinal inflammation] OR [VEO-IBD] OR [immune-mediated inflammatory diseases] OR [primary immunodeficiencies] OR [secondary immunodeficiencies] OR [autoinflammation] OR [inflammasome biology] OR [NLRP3] OR [cytokine signaling] OR [IL-1B] OR [IL-17] OR [IL-10] OR [IL-23] OR [IL-6] OR [pyroptosis] OR [trained immunity] OR [Nod-like receptors] OR [intestinal epithelial barrier] OR [Tregs] OR [macrophages] OR [dendritic cells] OR [organoids] OR [computational biology] OR [bioinformatics] OR [multiomics] OR [proteomics] OR [machine learning] OR [data integration] OR [benchmarking] OR [computational pathology]"
          
          # Define LLM filtering prompt directly in workflow - STRICT FILTERING
          filtering_prompt = """You are a highly selective lab manager at a specialized research lab. You must be EXTREMELY STRICT in selecting only the most relevant papers. The lab focuses specifically on: CORE RESEARCH AREAS (only these qualify): 1. Single-cell RNA sequencing methodology, analysis tools, or applications 2. Spatial transcriptomics techniques and computational methods 3. IBD/Crohn's disease/ulcerative colitis with molecular mechanisms 4. VEO-IBD (very early onset IBD) genetics and pathways 5. Inflammasome biology (NLRP3, NLRP1-13, Pyrin) with mechanistic insights 6. Primary/secondary immunodeficiencies with genetic or cellular analysis 7. Single-cell multiomics integration and computational methods 8. Intestinal epithelial barrier dysfunction at cellular/molecular level. STRICT EXCLUSION CRITERIA - IMMEDIATELY REJECT if paper is about: General machine learning without single-cell applications, Clinical studies without mechanistic insights, Cancer research (unless specifically intestinal/IBD-related), Drug discovery without IBD/immunodeficiency context, General proteomics/metabolomics without single-cell focus, Artificial intelligence applications outside genomics, General inflammatory diseases not related to IBD/gut, Methodological papers without single-cell relevance, Case reports or clinical observations without molecular data. DECISION RULES: Paper must directly address at least ONE core research area, Must provide mechanistic insights or novel methodology, Must be immediately applicable to lab's specific research, When in doubt REJECT - be highly conservative, Aim to select maximum 20-30 truly exceptional papers per day. You are reviewing research papers for daily digest. Be ruthlessly selective - only papers that would make lab members say 'we need to read this immediately' should pass. Answer ONLY 'yes' or 'no': Is this paper directly relevant and exceptional for our specialized research focus?"""
          
          config = {
              "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              
              # CRITICAL: Explicit database activation (like original PaperBee)
              "databases": ["pubmed", "arxiv", "biorxiv"],
              
              # Main query (used when no specific database query defined)
              "query": research_query,
              
              # Database-specific queries (optional but recommended)
              "query_biorxiv": research_query,
              "query_pubmed_arxiv": research_query,
              
              # BioRxiv-specific optimizations for stability
              "limit": 100,
              "limit_per_database": 25,  # Reduced for bioRxiv stability
              
              "LLM_FILTERING": True,
              "LLM_PROVIDER": "openai",
              "LANGUAGE_MODEL": "gemini-2.5-flash-lite",  # Upgraded to 2.5 Flash Lite
              "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
              "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
              "FILTERING_PROMPT": filtering_prompt,
              
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              },
              "TELEGRAM": {"is_posting_on": False},
              "ZULIP": {"is_posting_on": False}
          }
          
          with open("config.yml", "w") as f:
              yaml.dump(config, f, default_flow_style=False)
          
          # Debug output
          print("âœ… Generated config with explicit database activation:")
          print(f"ðŸ“Š Databases: {config['databases']}")
          print(f"ðŸ” Main query: {research_query[:100]}...")
          print(f"ðŸ§¬ BioRxiv query: {config.get('query_biorxiv', 'Using main query')[:100]}...")
          print(f"ðŸ“„ PubMed/ArXiv query: {config.get('query_pubmed_arxiv', 'Using main query')[:100]}...")
          print(f"ðŸ“Š Limits: total={config.get('limit', 'default')}, per_database={config.get('limit_per_database', 'default')}")
          print("Note: bioRxiv now explicitly activated with reduced limits for stability")
          print(f"Query contains {len(research_query.split(' OR '))} search terms")
          EOF
          
      - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "ðŸ Starting PaperBee Daily Digest"
          echo "ðŸ“… Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "ðŸ”§ NCBI timeout: 45s | Gemini 2.5 Flash Lite rate limit: 4.5s"
          echo "ðŸ› ï¸ Enhanced error handling: NCBI connection failures, XML parsing errors"
          echo "ðŸ’¬ Slack optimization: Batched papers to prevent 50-block limit errors"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config.yml
