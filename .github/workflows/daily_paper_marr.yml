# .github/workflows/daily_paper_marr.yml
name: PaperBee Daily Digest MarrPeng

on:
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6 AM UTC (8 AM German time during DST)
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 45s for stability)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=45/g' {} \;
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=30/timeout=45/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                with open(self.search_file_biorxiv) as papers_file:
                    articles_biorxiv_dict = json.load(papers_file)["papers"]
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"Found {len(articles_pub_arx_dict)} articles from PubMed/ArXiv")
            print(f"Found {len(articles_biorxiv_dict)} articles from bioRxiv")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied bioRxiv fix to {finder_file}")
                  else:
                      print(f"‚ÑπÔ∏è bioRxiv fix already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {finder_file}: {e}")
          EOF
          
          # Comprehensive NCBI error handling with multiple timeout patterns
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for comprehensive NCBI error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace various timeout patterns with comprehensive error handling
                  patterns_to_replace = [
                      ('fetch_response = requests.get(ncbi_url, timeout=45)', 
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"‚ö†Ô∏è NCBI connection failed: {str(e)[:50]}")
                          return None'''),
                      ('fetch_response = requests.get(ncbi_url, timeout=30)',
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"‚ö†Ô∏è NCBI connection failed: {str(e)[:50]}")
                          return None'''),
                      ('fetch_response = requests.get(ncbi_url, timeout=10)',
                       '''try:
                          fetch_response = requests.get(ncbi_url, timeout=45)
                      except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException) as e:
                          print(f"‚ö†Ô∏è NCBI connection failed: {str(e)[:50]}")
                          return None''')
                  ]
                  
                  changes_made = False
                  for old_pattern, new_pattern in patterns_to_replace:
                      if old_pattern in content and 'NCBI connection failed' not in content:
                          content = content.replace(old_pattern, new_pattern)
                          changes_made = True
                  
                  # Also handle XML parsing errors
                  xml_old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  xml_new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"‚ö†Ô∏è XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if xml_old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(xml_old_pattern, xml_new_pattern)
                      changes_made = True
                  
                  if changes_made:
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied comprehensive NCBI error handling to {utils_file}")
                  else:
                      print(f"‚ÑπÔ∏è NCBI error handling already applied or patterns not found in {utils_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini 2.5 Flash Lite rate limiting: 15 RPM = 4s minimum, use 4.5s for safety
                      time.sleep(4.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied Gemini 2.5 Flash Lite rate limiting to {llm_file}")
                  else:
                      print(f"‚ÑπÔ∏è Rate limiting already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {llm_file}: {e}")
          EOF
          
      - name: Run Multi-Group Pipeline
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID_2 }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN_2 }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID_2 }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN_2 }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          import json
          import subprocess
          import tempfile
          from pathlib import Path
          
          # Define three focused interest groups with specific queries and filters
          interest_groups = {
              "Happy Pixels": {
                  "emoji": "üñºÔ∏è",
                  "query": "[computer vision] OR [image analysis] OR [image processing] OR [image segmentation] OR [cell segmentation] OR [3D segmentation] OR [microscopy] OR [fluorescence microscopy] OR [quantitative microscopy] OR [AI microscopy] OR [biomedical image analysis] OR [medical imaging] OR [deep learning] AND [imaging] OR [machine learning] AND [microscopy] OR [foundation models] AND [imaging] OR [self-supervised learning] AND [images] OR [cell classification] OR [cell detection] OR [organoid detection] OR [organoid segmentation] OR [holography] OR [digital holography]",
                  "filter": "You are reviewing papers for the Happy Pixels research group focused on computational imaging and computer vision in biomedical contexts. ACCEPT papers about: Image analysis algorithms for biological/medical data, Computer vision methods for microscopy or medical imaging, Cell/organoid segmentation and detection methods, AI/ML approaches for image processing in biology/medicine, Novel imaging techniques with computational analysis, Foundation models or self-supervised learning for biological images, 3D imaging and analysis methods for biological samples. REJECT: Pure clinical imaging without computational methodology, General computer vision without biomedical application, Wet-lab protocols without imaging analysis, Papers focused solely on hardware without computational methods. Target: 8-12 highly relevant papers per day. Answer ONLY 'yes' or 'no': Is this paper relevant for computational imaging and computer vision in biomedical research?",
                  "limit": 100
              },
              "Omics and Dynamics": {
                  "emoji": "üß¨",
                  "query": "[single cell] OR [single-cell] OR [multi-omics] OR [multiomics] OR [spatial transcriptomics] OR [mathematical modeling] OR [mechanistic modeling] OR [clonal hematopoiesis] OR [CHIP] OR [clonal dynamics] OR [hematopoiesis] OR [computational biology] AND [dynamics] OR [systems biology] OR [mathematical] AND [biology] OR [modeling] AND [hematopoiesis] OR [omics] AND [computational] OR [transcriptomics] AND [analysis]",
                  "filter": "You are reviewing papers for the Omics and Dynamics research group focused on computational analysis of biological systems and dynamics. ACCEPT papers about: Single-cell omics analysis with computational methods, Multi-omics integration and computational approaches, Mathematical/mechanistic modeling of biological systems, Clonal hematopoiesis (CHIP) and clonal dynamics analysis, Spatial transcriptomics with computational analysis, Systems biology approaches to understand dynamics, Computational methods for omics data integration, Hematopoiesis modeling and dynamics analysis. REJECT: Pure experimental omics without computational analysis, Clinical studies without modeling components, General bioinformatics without novel methodology, Infectious disease studies without dynamics modeling. Target: 8-12 highly relevant papers per day. Answer ONLY 'yes' or 'no': Is this paper relevant for computational omics analysis and biological dynamics modeling?",
                  "limit": 100
              },
              "Histopathology": {
                  "emoji": "üî¨",
                  "query": "[computational pathology] OR [digital pathology] OR [pathology] AND [AI] OR [histopathology] OR [hematopathology] OR [pathology] AND [machine learning] OR [pathology] AND [deep learning] OR [morphological profiling] OR [cell painting] OR [representation learning] AND [pathology] OR [blood disorders] AND [computational] OR [hematology] AND [AI] OR [leukemia] AND [computational] OR [lymphoma] AND [computational] OR [myeloma] AND [computational]",
                  "filter": "You are reviewing papers for the Histopathology/Cytohistopathology research group focused on computational pathology and blood disorders. ACCEPT papers about: Computational pathology and digital pathology methods, AI/ML approaches for histopathological analysis, Hematopathology with computational components, Morphological profiling and representation learning for pathology, Blood disorders analysis with computational methods, AI-based diagnosis in pathology contexts, Machine learning for hematological malignancies, Computational analysis of pathological samples. REJECT: Pure clinical pathology without computational methods, General hematology without AI/computational components, Drug discovery without pathology focus, Basic cell biology without pathological context. Target: 8-12 highly relevant papers per day. Answer ONLY 'yes' or 'no': Is this paper relevant for computational pathology and hematopathological analysis?",
                  "limit": 100
              }
          }
          
          all_papers = []
          
          for group_name, group_config in interest_groups.items():
              print(f"\n{group_config['emoji']} Processing {group_name} group...")
              
              # Create config for this group
              config = {
                  "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
                  "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
                  "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
                  "LOCAL_ROOT_DIR": ".",
                  "databases": ["pubmed", "arxiv", "biorxiv"],
                  "query": group_config["query"],
                  "query_biorxiv": group_config["query"],
                  "query_pubmed_arxiv": group_config["query"],
                  "limit": group_config["limit"],
                  "limit_per_database": 8,
                  "LLM_FILTERING": True,
                  "LLM_PROVIDER": "openai",
                  "LANGUAGE_MODEL": "gemini-2.5-flash-lite",
                  "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
                  "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
                  "FILTERING_PROMPT": group_config["filter"],
                  "SLACK": {"is_posting_on": False},  # Disable individual posting
                  "TELEGRAM": {"is_posting_on": False},
                  "ZULIP": {"is_posting_on": False}
              }
              
              # Create temporary config file
              config_file = f"config_{group_name.lower().replace(' ', '_')}.yml"
              with open(config_file, "w") as f:
                  yaml.dump(config, f, default_flow_style=False)
              
              # Run PaperBee for this group
              try:
                  print(f"üîç Running query: {group_config['query'][:100]}...")
                  result = subprocess.run([
                      "paperbee", "post", 
                      "--config", config_file, 
                      "--since", "${{ steps.search_days.outputs.since_days }}"
                  ], capture_output=True, text=True, timeout=900)  # 15 min timeout per group
                  
                  if result.returncode == 0:
                      print(f"‚úÖ {group_name} completed successfully")
                      # Try to read results from Google Sheet or local files
                      # For now, we'll parse the output to extract paper count
                      output_lines = result.stdout.split('\n')
                      paper_count = 0
                      for line in output_lines:
                          if 'papers selected after filtering' in line.lower():
                              try:
                                  paper_count = int(line.split()[0])
                              except:
                                  pass
                      
                      all_papers.append({
                          "group": group_name,
                          "emoji": group_config["emoji"],
                          "count": paper_count,
                          "status": "success"
                      })
                  else:
                      print(f"‚ùå {group_name} failed: {result.stderr}")
                      all_papers.append({
                          "group": group_name,
                          "emoji": group_config["emoji"],
                          "count": 0,
                          "status": "failed"
                      })
                      
              except subprocess.TimeoutExpired:
                  print(f"‚è∞ {group_name} timed out")
                  all_papers.append({
                      "group": group_name,
                      "emoji": group_config["emoji"],
                      "count": 0,
                      "status": "timeout"
                  })
              except Exception as e:
                  print(f"üí• {group_name} error: {str(e)}")
                  all_papers.append({
                      "group": group_name,
                      "emoji": group_config["emoji"],
                      "count": 0,
                      "status": "error"
                  })
              
              # Clean up config file
              try:
                  os.remove(config_file)
              except:
                  pass
          
          # Send consolidated Slack message
          print(f"\nüì§ Sending consolidated Slack message...")
          
          # Create final summary config for Slack posting
          summary_config = {
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              }
          }
          
          # Send summary via custom Slack message
          import requests
          
          slack_token = os.environ.get("SLACK_BOT", "")
          slack_channel = os.environ.get("SLACK_CHANNEL", "")
          
          if slack_token and slack_channel:
              total_papers = sum(p["count"] for p in all_papers if p["status"] == "success")
              
              # Create structured message
              blocks = [
                  {
                      "type": "header",
                      "text": {
                          "type": "plain_text",
                          "text": f"ü©∏ MarrLab Daily Papers - {total_papers} papers found"
                      }
                  },
                  {
                      "type": "section",
                      "text": {
                          "type": "mrkdwn",
                          "text": f"*Daily research digest for {group_config.get('date', 'today')}*\nProcessed across three research focus areas:"
                      }
                  }
              ]
              
              # Add results for each group
              for paper_info in all_papers:
                  if paper_info["status"] == "success" and paper_info["count"] > 0:
                      blocks.append({
                          "type": "section",
                          "text": {
                              "type": "mrkdwn",
                              "text": f"{paper_info['emoji']} *{paper_info['group']}*: {paper_info['count']} papers\n_Check Google Sheet for detailed results_"
                          }
                      })
                  elif paper_info["status"] != "success":
                      blocks.append({
                          "type": "section",
                          "text": {
                              "type": "mrkdwn",
                              "text": f"{paper_info['emoji']} *{paper_info['group']}*: Processing {paper_info['status']}"
                          }
                      })
              
              # Add footer
              blocks.append({
                  "type": "context",
                  "elements": [
                      {
                          "type": "mrkdwn",
                          "text": f"ü§ñ Automated by PaperBee | üìä Total: {total_papers} papers | ‚è∞ {len(all_papers)} groups processed"
                      }
                  ]
              })
              
              # Send to Slack
              try:
                  response = requests.post(
                      "https://slack.com/api/chat.postMessage",
                      headers={"Authorization": f"Bearer {slack_token}"},
                      json={
                          "channel": slack_channel,
                          "blocks": blocks
                      }
                  )
                  
                  if response.status_code == 200 and response.json().get("ok"):
                      print("‚úÖ Consolidated Slack message sent successfully")
                  else:
                      print(f"‚ùå Slack API error: {response.text}")
                      
              except Exception as e:
                  print(f"üí• Slack sending error: {str(e)}")
          
          # Final summary
          print(f"\nüìã FINAL SUMMARY:")
          print(f"üî¢ Total papers found: {total_papers}")
          for paper_info in all_papers:
              print(f"  {paper_info['emoji']} {paper_info['group']}: {paper_info['count']} papers ({paper_info['status']})")
          
          EOF
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config_*.yml
