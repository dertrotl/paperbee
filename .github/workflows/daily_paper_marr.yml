# .github/workflows/research-papers.yml
name: PaperBee Daily Digest MarrPeng

on:
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6 AM UTC (8 AM German time during DST)
  workflow_dispatch:

jobs:
  fetch-papers:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e .
          
      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json
          
      - name: Determine search window
        id: search_days
        run: |
          DAY_OF_WEEK=$(date +%u)
          if [ "$DAY_OF_WEEK" -eq 1 ]; then
            echo "since_days=2" >> $GITHUB_OUTPUT
          else
            echo "since_days=1" >> $GITHUB_OUTPUT
          fi
          
      - name: Apply performance fixes
        run: |
          # Fix NCBI timeout issue (increase from 10s to 30s)
          find . -name "utils.py" -path "*/PaperBee/*" -exec sed -i 's/timeout=10/timeout=30/g' {} \;
          
          # Apply bioRxiv search fix
          python3 << 'EOF'
          import os
          
          # Find papers_finder.py and apply bioRxiv fix + debug
          finder_files = []
          for root, dirs, files in os.walk('.'):
              if 'papers_finder.py' in files and 'PaperBee' in root:
                  finder_files.append(os.path.join(root, 'papers_finder.py'))
          
          for finder_file in finder_files:
              try:
                  with open(finder_file, 'r') as f:
                      content = f.read()
                  
                  # Fix bioRxiv loading issue
                  old_pattern = '''            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            with open(self.search_file_biorxiv) as papers_file:
                articles_biorxiv_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  new_pattern = '''            articles_biorxiv_dict: List[Dict[str, Any]] = []
            if "biorxiv" in self.databases:
                findpapers.search(
                    self.search_file_biorxiv,
                    self.query_biorxiv,
                    self.since,
                    self.until,
                    self.limit,
                    self.limit_per_database,
                    ["biorxiv"],
                    verbose=False,
                )
                with open(self.search_file_biorxiv) as papers_file:
                    articles_biorxiv_dict = json.load(papers_file)["papers"]
            
            with open(self.search_file_pub_arx) as papers_file:
                articles_pub_arx_dict: List[Dict[str, Any]] = json.load(papers_file)["papers"]
            print(f"Found {len(articles)} articles from unified query")
            if len(articles) > 0:
                print(f"First few titles: {[art.get('title', 'No title')[:50] for art in articles[:3]]}")
                # Show databases distribution
                databases_count = {}
                for art in articles:
                    for db in art.get('databases', []):
                        databases_count[db] = databases_count.get(db, 0) + 1
                print(f"üìä Articles by database: {databases_count}")
            else:
                print("‚ö†Ô∏è  NO ARTICLES FOUND - this might be a search/API issue!")
            articles = articles_pub_arx_dict + articles_biorxiv_dict'''
                  
                  if old_pattern in content:
                      content = content.replace(old_pattern, new_pattern)
                  
                  # Add debug to LLM filtering section - Fix the pattern matching
                  old_llm_pattern = '''if self.llm_filtering:
            llm_filter = LLMFilter(
                processed_articles,
                llm_provider=self.llm_provider,
                model=self.model,
                filtering_prompt=self.filtering_prompt,
                OPENAI_API_KEY=self.OPENAI_API_KEY,
                OPENAI_BASE_URL=self.OPENAI_BASE_URL,
            )
            processed_articles = llm_filter.filter_articles()
            self.logger.info(f"Filtered down to {len(processed_articles)} articles using LLM.")'''
                  
                  new_llm_pattern = '''if self.llm_filtering:
            print(f"üß† LLM filtering ENABLED: Processing {len(processed_articles)} articles")
            print(f"üìä Article breakdown by source before LLM: {processed_articles['Source'].value_counts().to_dict()}")
            llm_filter = LLMFilter(
                processed_articles,
                llm_provider=self.llm_provider,
                model=self.model,
                filtering_prompt=self.filtering_prompt,
                OPENAI_API_KEY=self.OPENAI_API_KEY,
                OPENAI_BASE_URL=self.OPENAI_BASE_URL,
            )
            processed_articles = llm_filter.filter_articles()
            print(f"‚úÖ LLM filtering completed: {len(processed_articles)} articles remain")
            self.logger.info(f"Filtered down to {len(processed_articles)} articles using LLM.")
        else:
            print("‚ö†Ô∏è  LLM filtering DISABLED - all articles pass through unfiltered")'''
                  
                  # Also add a more general fallback pattern
                  if old_llm_pattern not in content:
                      # Try alternative pattern without indentation
                      old_llm_pattern = '''        if self.llm_filtering:
            llm_filter = LLMFilter('''
                      new_llm_pattern = '''        if self.llm_filtering:
            print(f"üß† LLM filtering ENABLED: Processing {len(processed_articles)} articles")
            print(f"üìä Article breakdown by source before LLM: {processed_articles['Source'].value_counts().to_dict()}")
            llm_filter = LLMFilter('''
                  
                  if old_llm_pattern in content and 'üß† LLM filtering ENABLED' not in content:
                      content = content.replace(old_llm_pattern, new_llm_pattern)
                      
                      # Also add the end part
                      old_end_pattern = '''            processed_articles = llm_filter.filter_articles()
            self.logger.info(f"Filtered down to {len(processed_articles)} articles using LLM.")'''
                      new_end_pattern = '''            processed_articles = llm_filter.filter_articles()
            print(f"‚úÖ LLM filtering completed: {len(processed_articles)} articles remain")
            self.logger.info(f"Filtered down to {len(processed_articles)} articles using LLM.")
        else:
            print("‚ö†Ô∏è  LLM filtering DISABLED - all articles pass through unfiltered")'''
                      
                      if old_end_pattern in content:
                          content = content.replace(old_end_pattern, new_end_pattern)
                      
                      with open(finder_file, 'w') as f:
                          f.write(content)
                      
                      # Force add debug right after writing
                      with open(finder_file, 'r') as f:
                          content = f.read()
                      
                      # Add debug to track LLM filtering status
                      if 'print("üîç Debug: LLM filtering check")' not in content:
                          content = content.replace(
                              'def find_and_process_papers(self) -> pd.DataFrame:',
                              '''def find_and_process_papers(self) -> pd.DataFrame:
        print("üîç Debug: LLM filtering check")
        print(f"self.llm_filtering = {self.llm_filtering}")'''
                          )
                          
                          with open(finder_file, 'w') as f:
                              f.write(content)
                      
                      print(f"‚úÖ Applied bioRxiv fix and LLM debug to {finder_file}")
                  else:
                      print(f"‚ÑπÔ∏è bioRxiv fix and debug already applied or pattern not found in {finder_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {finder_file}: {e}")
          EOF
          
          # Fix XML parsing errors in DOI extraction
          python3 << 'EOF'
          import os
          import re
          
          # Find and patch utils.py for XML error handling
          utils_files = []
          for root, dirs, files in os.walk('.'):
              if 'utils.py' in files and 'PaperBee' in root:
                  utils_files.append(os.path.join(root, 'utils.py'))
          
          for utils_file in utils_files:
              try:
                  with open(utils_file, 'r') as f:
                      content = f.read()
                  
                  # Replace the problematic XML parsing line
                  old_pattern = 'root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing'
                  new_pattern = '''try:
                      root = ET.fromstring(fetch_response.content)  # Using defusedxml for parsing
                  except (ET.ParseError, Exception) as e:
                      print(f"‚ö†Ô∏è XML parsing failed, skipping DOI extraction: {str(e)[:50]}")
                      return None'''
                  
                  if old_pattern in content and 'XML parsing failed' not in content:
                      content = content.replace(old_pattern, new_pattern)
                      
                      with open(utils_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied XML error handling to {utils_file}")
                  else:
                      print(f"‚ÑπÔ∏è XML error handling already applied or pattern not found in {utils_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {utils_file}: {e}")
          EOF
          
          # Add Gemini rate limiting to LLM filtering
          python3 << 'EOF'
          import os
          import re
          
          # Find the llm_filtering.py file
          llm_files = []
          for root, dirs, files in os.walk('.'):
              if 'llm_filtering.py' in files and 'PaperBee' in root:
                  llm_files.append(os.path.join(root, 'llm_filtering.py'))
          
          for llm_file in llm_files:
              try:
                  with open(llm_file, 'r') as f:
                      content = f.read()
                  
                  # Add rate limiting import at the top
                  if 'import time' not in content:
                      content = 'import time\n' + content
                  
                  # Add rate limiting before the client.chat.completions.create call
                  old_pattern = 'response = client.chat.completions.create('
                  new_pattern = '''# Gemini rate limiting: 30 RPM = 2 seconds minimum, use 3.5s for safety
                      time.sleep(3.5)
                      response = client.chat.completions.create('''
                  
                  if old_pattern in content and 'time.sleep(3.5)' not in content:
                      content = content.replace(old_pattern, new_pattern)
                  
                  # Add debug logging to LLM filtering
                  old_debug_pattern = 'def filter_articles(self):'
                  new_debug_pattern = '''def filter_articles(self):
          print(f"üß† LLM Filtering: Starting with {len(self.articles)} articles")
          print(f"Article sources: {self.articles['Source'].value_counts().to_dict()}")'''
                  
                  if old_debug_pattern in content and 'üß† LLM Filtering' not in content:
                      content = content.replace(old_debug_pattern, new_debug_pattern)
                  
                  # Add debug info in is_relevant method
                  old_relevant_pattern = 'def is_relevant('
                  new_relevant_pattern = '''def is_relevant('''
                  
                  # Add debug after successful LLM call
                  old_success_pattern = 'if response.choices[0].message.content.strip().lower() == "yes":'
                  new_success_pattern = '''print(f"ü§ñ LLM response for '{title[:30]}...': {response.choices[0].message.content.strip()}")
                  if response.choices[0].message.content.strip().lower() == "yes":'''
                  
                  if old_success_pattern in content and 'ü§ñ LLM response' not in content:
                      content = content.replace(old_success_pattern, new_success_pattern)
                  
                  # Add debug at end of filter_articles
                  old_return_pattern = 'return filtered_articles'
                  new_return_pattern = '''print(f"üß† LLM Filtering: Filtered down to {len(filtered_articles)} articles")
          print(f"Final article sources: {filtered_articles['Source'].value_counts().to_dict() if len(filtered_articles) > 0 else 'No articles'}")
          return filtered_articles'''
                  
                  if old_return_pattern in content and 'üß† LLM Filtering: Filtered down to' not in content:
                      content = content.replace(old_return_pattern, new_return_pattern)
                      
                      with open(llm_file, 'w') as f:
                          f.write(content)
                      
                      print(f"‚úÖ Applied Gemini rate limiting and debug to {llm_file}")
                  else:
                      print(f"‚ÑπÔ∏è Rate limiting and debug already applied or pattern not found in {llm_file}")
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Error processing {llm_file}: {e}")
          EOF
          
      - name: Create config
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SPREADSHEET_ID_2 }}
          NCBI_KEY: ${{ secrets.NCBI_API_KEY }}
          GEMINI_KEY: ${{ secrets.GEMINI_API_KEY }}
          SLACK_BOT: ${{ secrets.SLACK_BOT_TOKEN_2 }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL_ID_2 }}
          SLACK_APP: ${{ secrets.SLACK_APP_TOKEN_2 }}
        run: |
          python3 << 'EOF'
          import yaml
          import os
          
          # Use single query for all databases (original working version)
          research_query = "[hematopoiesis] OR [haematopoiesis] OR [hematology] OR [haematology] OR [blood disorders] OR [computational pathology] OR [cell segmentation] OR [segmentation] OR [3D segmentation] OR [single cell] OR [single-cell] OR [machine learning] OR [deep learning] OR [artificial intelligence] OR [computer vision] OR [image analysis] OR [microscopy] OR [computational biology] OR [multi-omics] OR [multiomics] OR [foundation models] OR [self-supervised learning] OR [cell classification] OR [cell detection] OR [image processing] OR [biomedical image analysis] OR [medical imaging] OR [AI microscopy] OR [quantitative microscopy] OR [fluorescence microscopy] OR [vision language model] OR [large language models] OR [mathematical modeling] OR [mechanistic modeling] OR [drug target identification] OR [spatial transcriptomics] OR [pathology] OR [hematopathology] OR [blood cell] OR [leukemia] OR [lymphoma] OR [myeloma] OR [CHIP] OR [clonal hematopoiesis] OR [LLM] OR [language model]"
          
          # Define LLM filtering prompt directly in workflow (less restrictive)
          filtering_prompt = """You are a lab manager at a research lab focusing on computational hematology, blood disorders, and microscopy image analysis, computational pathology and bioinformatics. The lab develops machine learning algorithms for cell segmentation, computational pathology, and single-cell profiling in hematological contexts. Key research areas include clonal hematopoiesis (CHIP), clonal dynamics, mechanistic mathematical modeling of haematopoiesis, and multi-omics approaches to blood cell production and disorders. The lab works extensively with foundation models, self-supervised learning, and multiple instance learning for analyzing microscopic images including bright-field, fluorescence, cryo-electron, and extended depth-of-field microscopy. Research focuses on developing AI-based algorithms for cell detection, classification, and quantification to advance diagnosis and treatment of severe blood disorders. Lab members are interested in computational hematopathology, single cell-based big data analysis for drug target identification, mechanistic modeling of time-resolved hematological data, and large language models/agents in healthcare applications. The lab combines AI with biomedical knowledge to classify individual cells and patients, with particular emphasis on quantitative microscopy image processing and custom machine learning methods for life scientists and pathologists. 

You should be INCLUSIVE rather than exclusive in your filtering. Papers that are even tangentially related to machine learning, computational biology, single-cell analysis, hematology, pathology, or image analysis should be considered relevant. When in doubt, choose 'yes' rather than 'no'.

You are reviewing a list of research papers to determine if they are relevant to your lab. Please answer 'yes' or 'no' to the following question: Is the following research paper relevant?"""
          
          config = {
              "GOOGLE_SPREADSHEET_ID": os.environ.get("GOOGLE_SHEET_ID", ""),
              "GOOGLE_CREDENTIALS_JSON": "./google-credentials.json",
              "NCBI_API_KEY": os.environ.get("NCBI_KEY", ""),
              "LOCAL_ROOT_DIR": ".",
              
              # Use single query for all databases (original working approach)
              "query": research_query,
              
              "LLM_FILTERING": True,
              "LLM_PROVIDER": "openai",
              "LANGUAGE_MODEL": "gemini-2.0-flash-lite",
              "OPENAI_API_KEY": os.environ.get("GEMINI_KEY", ""),
              "OPENAI_BASE_URL": "https://generativelanguage.googleapis.com/v1beta/openai/",
              "FILTERING_PROMPT": filtering_prompt,
              
              "SLACK": {
                  "is_posting_on": True,
                  "bot_token": os.environ.get("SLACK_BOT", ""),
                  "channel_id": os.environ.get("SLACK_CHANNEL", ""),
                  "app_token": os.environ.get("SLACK_APP", "")
              },
              "TELEGRAM": {"is_posting_on": False},
              "ZULIP": {"is_posting_on": False}
          }
          
          with open("config.yml", "w") as f:
              yaml.dump(config, f, default_flow_style=False)
          
          # Debug output
          print("Generated config:")
          print(f"Query: {research_query}")
          print("Using same query for all databases (PubMed, ArXiv, bioRxiv)")
          print("Note: Broad queries for maximum coverage - LLM will filter for relevance")
          print(f"Query contains {len(research_query.split(' OR '))} search terms")
          print(f"LLM_FILTERING enabled: {config['LLM_FILTERING']}")
          print(f"LLM_PROVIDER: {config['LLM_PROVIDER']}")
          print(f"LANGUAGE_MODEL: {config['LANGUAGE_MODEL']}")
          EOF
          
      - name: Run PaperBee
        env:
          NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}
        run: |
          echo "üî¨ Starting PaperBee Research Digest"
          echo "üìÖ Search window: ${{ steps.search_days.outputs.since_days }} days"
          echo "üîß NCBI timeout: 30s | Gemini rate limit: 3.5s"
          paperbee post --config config.yml --since ${{ steps.search_days.outputs.since_days }}
          
      - name: Clean up
        if: always()
        run: |
          rm -f google-credentials.json config.yml
